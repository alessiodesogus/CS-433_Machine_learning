{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions : \n",
    "1. The two columns represent the constant term (intercept) and the height feature.\n",
    "2. Each row corresponds to a data sample\n",
    "3. This is typically used to add a bias term (intercept) to the linear regression model. Each one corresponds to the constant term in the linear equation.\n",
    "4. y.shape would be (3,) and tx.shape would be (3,2). Finally $\\tilde{X}_{32}$ would represent the height of the third people\n",
    "5. See the comment in the file \"AD_comments.ipynb\"\n",
    "6. See above, yes they make sense !\n",
    "7. See the sheet in the Ipad\n",
    "\n",
    "\n",
    "Fill in the `compute_loss` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    N = tx.shape[0]\n",
    "    e = y - np.dot(tx, w)\n",
    "    MSE = (1/2*N) * np.dot(e.T,e) # AD : alternative : np.dot(e,e) = np.linalg.norm(e)**2\n",
    "    return MSE\n",
    "\n",
    "    # TODO: compute loss by MSE\n",
    "    # ***************************************************\n",
    "\n",
    "    # AD : Good Implementation - See the results in the section Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269448336588.70837"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_test = np.array([1, 2])\n",
    "compute_loss(y, tx, w_test.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "\n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    # ***************************************************\n",
    "    for i in range(len(grid_w0)) :  \n",
    "        for j in range(len(grid_w1)) :\n",
    "            w_grid = np.array([grid_w0[i], grid_w1[j]]) # AD : I create an array with the value of grid_w0 and grid_w1\n",
    "            losses[i,j] = compute_loss(y, tx, w_grid.T)\n",
    "    return losses\n",
    "\n",
    "    # TODO: compute loss for each combination of w0 and w1.\n",
    "    # ***************************************************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=1561008565.2488797, w0*=73.36683417085428, w1*=12.8140703517588, execution time=2.757 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=200) #AD : Change here the number of param. to 50 instead.\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\n",
    "    \"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "        l=loss_star, w0=w0_star, w1=w1_star, t=execution_time\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions : \n",
    "- b) No with a grid space of 10, it doesn't like a good estimate, the red line shloud be more over than that (in this situation we underestimate). But with a grid space of 50, we have a pretty good fitting.\n",
    "- c) Discussion :\n",
    "1. We need a fine grid (high grid space) - coarse mean \"grossier\" in french\n",
    "2. With a grid space of 200 the fitting become more and more accurate \n",
    "3. It really affect it, I can't run my code with a grid space of 1000 for example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    N = y.shape[0]\n",
    "    e = y - np.dot(tx, w) #same as before\n",
    "    grad = (-1/N)*np.dot(tx.T,e)\n",
    "    return np.transpose(grad)\n",
    "    # TODO: compute gradient vector\n",
    "    # ***************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26.706078    6.52028757]\n",
      "27.49052112929254\n",
      "[-23.293922    -3.47971243]\n",
      "23.552392678247738\n"
     ]
    }
   ],
   "source": [
    "#Compute the gradient of :\n",
    "w1 = np.array([100, 20])\n",
    "grad1 = compute_gradient(y, tx, w1)\n",
    "print(grad1)\n",
    "print(np.linalg.norm(grad1))\n",
    "\n",
    "w2 = np.array([50, 10])\n",
    "grad2 = compute_gradient(y, tx, w2)\n",
    "print(grad2)\n",
    "print(np.linalg.norm(grad2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions :\n",
    "- b) ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        gradient = compute_gradient(y,tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        # ***************************************************\n",
    "        w = w - gamma*gradient\n",
    "        \n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=279223671275.9167, w0=7.329392200210504, w1=1.3479712434989137\n",
      "GD iter. 1/49: loss=226463505603.00037, w0=13.925845180399968, w1=2.561145362647917\n",
      "GD iter. 2/49: loss=183727771407.93814, w0=19.86265286257048, w1=3.6530020698820245\n",
      "GD iter. 3/49: loss=149111826709.93768, w0=25.205779776523947, w1=4.635673106392713\n",
      "GD iter. 4/49: loss=121072911504.5573, w0=30.014593999082074, w1=5.520077039252335\n",
      "GD iter. 5/49: loss=98361390188.19919, w0=34.34252679938438, w1=6.316040578825998\n",
      "GD iter. 6/49: loss=79965057921.94913, w0=38.23766631965647, w1=7.032407764442294\n",
      "GD iter. 7/49: loss=65064028786.286514, w0=41.743291887901336, w1=7.67713823149696\n",
      "GD iter. 8/49: loss=52994195186.399864, w0=44.89835489932172, w1=8.257395651846164\n",
      "GD iter. 9/49: loss=43217629970.49167, w0=47.73791160960007, w1=8.779627330160444\n",
      "GD iter. 10/49: loss=35298612145.606, w0=50.29351264885058, w1=9.249635840643297\n",
      "GD iter. 11/49: loss=28884207707.44862, w0=52.59355358417604, w1=9.672643500077871\n",
      "GD iter. 12/49: loss=23688540112.54114, w0=54.663590425968955, w1=10.053350393568985\n",
      "GD iter. 13/49: loss=19480049360.66609, w0=56.52662358358258, w1=10.395986597710987\n",
      "GD iter. 14/49: loss=16071171851.647291, w0=58.20335342543484, w1=10.70435918143879\n",
      "GD iter. 15/49: loss=13309981069.34206, w0=59.712410283101875, w1=10.981894506793811\n",
      "GD iter. 16/49: loss=11073416535.674831, w0=61.070561455002206, w1=11.231676299613333\n",
      "GD iter. 17/49: loss=9261799263.40437, w0=62.2928975097125, w1=11.456479913150902\n",
      "GD iter. 18/49: loss=7794389272.865301, w0=63.39299995895177, w1=11.658803165334716\n",
      "GD iter. 19/49: loss=6605787180.528657, w0=64.38309216326711, w1=11.840894092300147\n",
      "GD iter. 20/49: loss=5643019485.735974, w0=65.27417514715091, w1=12.004775926569035\n",
      "GD iter. 21/49: loss=4863177652.953899, w0=66.07614983264634, w1=12.152269577411035\n",
      "GD iter. 22/49: loss=4231505768.400421, w0=66.79792704959222, w1=12.285013863168833\n",
      "GD iter. 23/49: loss=3719851541.9121017, w0=67.44752654484351, w1=12.404483720350854\n",
      "GD iter. 24/49: loss=3305411618.4565635, w0=68.03216609056967, w1=12.512006591814673\n",
      "GD iter. 25/49: loss=2969715280.4575777, w0=68.55834168172322, w1=12.60877717613211\n",
      "GD iter. 26/49: loss=2697801246.6783996, w0=69.03189971376142, w1=12.695870702017803\n",
      "GD iter. 27/49: loss=2477550879.3172617, w0=69.45810194259579, w1=12.774254875314927\n",
      "GD iter. 28/49: loss=2299148081.754743, w0=69.84168394854673, w1=12.84480063128234\n",
      "GD iter. 29/49: loss=2154641815.7290998, w0=70.18690775390257, w1=12.90829181165301\n",
      "GD iter. 30/49: loss=2037591740.24833, w0=70.49760917872284, w1=12.965433873986614\n",
      "GD iter. 31/49: loss=1942781179.1089053, w0=70.77724046106107, w1=13.016861730086857\n",
      "GD iter. 32/49: loss=1865984624.5859733, w0=71.02890861516548, w1=13.063146800577076\n",
      "GD iter. 33/49: loss=1803779415.4223976, w0=71.25540995385946, w1=13.104803364018274\n",
      "GD iter. 34/49: loss=1753393195.9998991, w0=71.45926115868403, w1=13.14229427111535\n",
      "GD iter. 35/49: loss=1712580358.2676768, w0=71.64272724302614, w1=13.17603608750272\n",
      "GD iter. 36/49: loss=1679521959.7045774, w0=71.80784671893404, w1=13.206403722251352\n",
      "GD iter. 37/49: loss=1652744656.8684666, w0=71.95645424725116, w1=13.23373459352512\n",
      "GD iter. 38/49: loss=1631055041.571217, w0=72.09020102273657, w1=13.258332377671513\n",
      "GD iter. 39/49: loss=1613486453.1804438, w0=72.21057312067343, w1=13.280470383403266\n",
      "GD iter. 40/49: loss=1599255896.5839174, w0=72.3189080088166, w1=13.300394588561844\n",
      "GD iter. 41/49: loss=1587729145.740732, w0=72.41640940814547, w1=13.318326373204563\n",
      "GD iter. 42/49: loss=1578392477.5577514, w0=72.50416066754144, w1=13.334464979383013\n",
      "GD iter. 43/49: loss=1570829776.329537, w0=72.58313680099782, w1=13.348989724943616\n",
      "GD iter. 44/49: loss=1564703988.3346832, w0=72.65421532110855, w1=13.362061995948158\n",
      "GD iter. 45/49: loss=1559742100.0588522, w0=72.71818598920821, w1=13.373827039852248\n",
      "GD iter. 46/49: loss=1555722970.5554295, w0=72.77575959049791, w1=13.384415579365928\n",
      "GD iter. 47/49: loss=1552467475.6576564, w0=72.82757583165863, w1=13.39394526492824\n",
      "GD iter. 48/49: loss=1549830524.7904606, w0=72.8742104487033, w1=13.40252198193432\n",
      "GD iter. 49/49: loss=1547694594.5880313, w0=72.91618160404349, w1=13.410241027239794\n",
      "GD: execution time=0.014 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4066194f30404aa404331ebf21cdb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions :\n",
    "- c) \n",
    "1. Yes, the cost is being minimized over the step !\n",
    "2. Yes, the algorithm is converging and pretty fast (in 4-5 step it stabilize in the same order of magnitude)\n",
    "3. The algorithm is converging so the values are good\n",
    "\n",
    "- d) \n",
    "1. With the step size : 0.001, 0.01, 2, 2.5 the algorithm doesn't converges, it convergences only with the step size : 0,5, 1\n",
    "2. Yes the procedure convergence for the three options but in different ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    stoch_gradient = compute_gradient(y, tx, w)\n",
    "    return stoch_gradient\n",
    "    # TODO: implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    # ***************************************************\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1) :\n",
    "        \n",
    "            grad = compute_stoch_gradient(y_batch, tx_batch, w)\n",
    "            w = w - gamma*grad\n",
    "            loss = compute_loss(y, tx, w)\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "            \n",
    "        # TODO: implement stochastic gradient descent.\n",
    "        # ***************************************************\n",
    "\n",
    "        print(\n",
    "            \"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=220486351162.83447, w0=8.054996032990458, w1=2.396491517069974\n",
      "SGD iter. 1/49: loss=167666082045.40637, w0=16.017693557282485, w1=7.000243297247\n",
      "SGD iter. 2/49: loss=134949704016.42628, w0=21.995652761100448, w1=7.420844762869572\n",
      "SGD iter. 3/49: loss=114645220332.40938, w0=26.485195126595563, w1=5.0490707509609125\n",
      "SGD iter. 4/49: loss=88248122517.63344, w0=31.66328826893615, w1=14.519428545154504\n",
      "SGD iter. 5/49: loss=72399101728.21513, w0=35.87759136398702, w1=17.630430494031767\n",
      "SGD iter. 6/49: loss=67713044160.63694, w0=38.22541886944983, w1=23.159030360856594\n",
      "SGD iter. 7/49: loss=56929878912.66541, w0=41.516755549124156, w1=23.381100960604314\n",
      "SGD iter. 8/49: loss=43874096554.05432, w0=45.027140972030125, w1=20.386176475380157\n",
      "SGD iter. 9/49: loss=36961748721.95535, w0=48.163280206772036, w1=22.249777523464854\n",
      "SGD iter. 10/49: loss=33563577600.024963, w0=50.14364716933296, w1=23.70539341133251\n",
      "SGD iter. 11/49: loss=21838908771.028008, w0=53.75346615953431, w1=18.396722212022112\n",
      "SGD iter. 12/49: loss=20272405669.513256, w0=54.73213809233194, w1=18.969385989900022\n",
      "SGD iter. 13/49: loss=18488001806.48784, w0=56.18442459873565, w1=20.28069464318433\n",
      "SGD iter. 14/49: loss=15429537513.46263, w0=58.21389435929329, w1=20.579835027143535\n",
      "SGD iter. 15/49: loss=13538248677.311079, w0=59.444974237480466, w1=20.42232315583884\n",
      "SGD iter. 16/49: loss=10432620045.845781, w0=61.32039422837649, w1=19.35468118648077\n",
      "SGD iter. 17/49: loss=9090272334.392776, w0=62.14022428993539, w1=18.640012648710606\n",
      "SGD iter. 18/49: loss=8343787662.096576, w0=62.95572893392652, w1=18.885796192140347\n",
      "SGD iter. 19/49: loss=6594269865.474515, w0=64.33118604735904, w1=18.038548446223935\n",
      "SGD iter. 20/49: loss=6047974043.622406, w0=64.8882045694969, w1=17.899171630177353\n",
      "SGD iter. 21/49: loss=5096851903.309274, w0=65.87416607136468, w1=17.493748341251933\n",
      "SGD iter. 22/49: loss=4412615691.983394, w0=66.48361834751965, w1=16.81142404230861\n",
      "SGD iter. 23/49: loss=4558753765.398758, w0=65.84961843702933, w1=15.712568006071788\n",
      "SGD iter. 24/49: loss=4396847747.013852, w0=66.31519996532286, w1=16.388770347088244\n",
      "SGD iter. 25/49: loss=4834936343.131901, w0=65.8030510397102, w1=16.612411486881314\n",
      "SGD iter. 26/49: loss=4121514064.4651074, w0=66.69115305820499, w1=16.31906937364793\n",
      "SGD iter. 27/49: loss=4099317818.140176, w0=66.72413212568797, w1=16.317394870511126\n",
      "SGD iter. 28/49: loss=2917962813.537039, w0=68.10129336349061, w1=14.269705064398767\n",
      "SGD iter. 29/49: loss=2213951580.0364733, w0=69.62776221648889, w1=13.221781650861763\n",
      "SGD iter. 30/49: loss=2173734485.106888, w0=69.74115537100617, w1=13.195523713488178\n",
      "SGD iter. 31/49: loss=1956018033.84837, w0=70.45124191900524, w1=13.997162804063674\n",
      "SGD iter. 32/49: loss=1955311039.3426335, w0=70.45323524910513, w1=13.994439372815181\n",
      "SGD iter. 33/49: loss=1977687468.8970292, w0=70.37707164365933, w1=14.003122051557247\n",
      "SGD iter. 34/49: loss=2259498997.278411, w0=72.22088439526371, w1=9.837351318595818\n",
      "SGD iter. 35/49: loss=2263828542.215591, w0=72.3077024338694, w1=9.80110317741876\n",
      "SGD iter. 36/49: loss=1970253549.4805343, w0=73.86903798614422, w1=10.598300132513977\n",
      "SGD iter. 37/49: loss=1934619659.0733774, w0=74.36145105277102, w1=10.875677264375323\n",
      "SGD iter. 38/49: loss=1913320086.7327478, w0=74.61821550644355, w1=11.083700600426587\n",
      "SGD iter. 39/49: loss=1741087504.1641574, w0=73.31387474328898, w1=11.46735654356636\n",
      "SGD iter. 40/49: loss=1731062607.2642932, w0=73.50886762452666, w1=11.529513781767047\n",
      "SGD iter. 41/49: loss=1776684423.1643755, w0=73.65150586340239, w1=11.327028875389892\n",
      "SGD iter. 42/49: loss=1848412455.9892447, w0=73.34911013244485, w1=10.99105262448949\n",
      "SGD iter. 43/49: loss=1707621799.2235837, w0=74.18429438712775, w1=11.871018162356082\n",
      "SGD iter. 44/49: loss=1729698024.291088, w0=73.8586020236136, w1=11.60799631830035\n",
      "SGD iter. 45/49: loss=1726102889.930358, w0=73.38780751510794, w1=11.545425080070817\n",
      "SGD iter. 46/49: loss=1711793000.9553478, w0=73.21474281191477, w1=11.620192352470726\n",
      "SGD iter. 47/49: loss=1654795546.137019, w0=72.3298257224438, w1=12.298757927556757\n",
      "SGD iter. 48/49: loss=1657165672.7314756, w0=71.97129613628553, w1=12.690916811246428\n",
      "SGD iter. 49/49: loss=1910056140.6668682, w0=70.66428923697738, w1=12.762510513460075\n",
      "SGD: execution time=0.052 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790d2e122ef7406a95400ecac7df4281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses,\n",
    "        sgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions :\n",
    "- For the SGD the computational cost is cheaper but it takes more iteration to converge to the optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=False)\n",
    "# height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "# ***************************************************\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200,), (200, 2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=113170888.97753663, w0=51.54259072181176, w1=10.132993413506084\n",
      "GD iter. 1/49: loss=10682001.035117716, w0=67.0053679383553, w1=13.172891437557825\n",
      "GD iter. 2/49: loss=1458001.1203000185, w0=71.64420110331838, w1=14.084860844773322\n",
      "GD iter. 3/49: loss=627841.1279664254, w0=73.03585105280729, w1=14.358451666937965\n",
      "GD iter. 4/49: loss=553126.728656403, w0=73.45334603765397, w1=14.440528913587356\n",
      "GD iter. 5/49: loss=546402.4327185008, w0=73.57859453310797, w1=14.46515208758217\n",
      "GD iter. 6/49: loss=545797.2460840897, w0=73.61616908174418, w1=14.472539039780616\n",
      "GD iter. 7/49: loss=545742.7792869926, w0=73.62744144633503, w1=14.474755125440149\n",
      "GD iter. 8/49: loss=545737.877275254, w0=73.63082315571229, w1=14.47541995113801\n",
      "GD iter. 9/49: loss=545737.4360941977, w0=73.63183766852546, w1=14.475619398847368\n",
      "GD iter. 10/49: loss=545737.3963879024, w0=73.63214202236942, w1=14.475679233160175\n",
      "GD iter. 11/49: loss=545737.3928143359, w0=73.6322333285226, w1=14.475697183454017\n",
      "GD iter. 12/49: loss=545737.392492715, w0=73.63226072036856, w1=14.47570256854217\n",
      "GD iter. 13/49: loss=545737.3924637692, w0=73.63226893792235, w1=14.475704184068615\n",
      "GD iter. 14/49: loss=545737.3924611638, w0=73.63227140318848, w1=14.475704668726548\n",
      "GD iter. 15/49: loss=545737.3924609293, w0=73.63227214276833, w1=14.47570481412393\n",
      "GD iter. 16/49: loss=545737.3924609083, w0=73.63227236464228, w1=14.475704857743143\n",
      "GD iter. 17/49: loss=545737.3924609064, w0=73.63227243120446, w1=14.475704870828908\n",
      "GD iter. 18/49: loss=545737.3924609063, w0=73.63227245117312, w1=14.475704874754637\n",
      "GD iter. 19/49: loss=545737.3924609063, w0=73.63227245716372, w1=14.475704875932356\n",
      "GD iter. 20/49: loss=545737.3924609061, w0=73.6322724589609, w1=14.475704876285672\n",
      "GD iter. 21/49: loss=545737.3924609062, w0=73.63227245950004, w1=14.475704876391665\n",
      "GD iter. 22/49: loss=545737.3924609063, w0=73.63227245966179, w1=14.475704876423464\n",
      "GD iter. 23/49: loss=545737.3924609062, w0=73.63227245971032, w1=14.475704876433003\n",
      "GD iter. 24/49: loss=545737.3924609063, w0=73.63227245972487, w1=14.475704876435865\n",
      "GD iter. 25/49: loss=545737.3924609063, w0=73.63227245972924, w1=14.475704876436724\n",
      "GD iter. 26/49: loss=545737.3924609062, w0=73.63227245973054, w1=14.475704876436982\n",
      "GD iter. 27/49: loss=545737.3924609062, w0=73.63227245973094, w1=14.47570487643706\n",
      "GD iter. 28/49: loss=545737.3924609063, w0=73.63227245973106, w1=14.475704876437083\n",
      "GD iter. 29/49: loss=545737.3924609063, w0=73.6322724597311, w1=14.475704876437089\n",
      "GD iter. 30/49: loss=545737.3924609063, w0=73.63227245973111, w1=14.47570487643709\n",
      "GD iter. 31/49: loss=545737.3924609063, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 32/49: loss=545737.3924609063, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 33/49: loss=545737.3924609063, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 34/49: loss=545737.3924609063, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 35/49: loss=545737.3924609063, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 36/49: loss=545737.3924609063, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 37/49: loss=545737.3924609063, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 38/49: loss=545737.3924609063, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 39/49: loss=545737.3924609063, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 40/49: loss=545737.3924609063, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 41/49: loss=545737.3924609063, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 42/49: loss=545737.3924609063, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 43/49: loss=545737.3924609063, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 44/49: loss=545737.3924609063, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 45/49: loss=545737.3924609063, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 46/49: loss=545737.3924609063, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 47/49: loss=545737.3924609063, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 48/49: loss=545737.3924609063, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 49/49: loss=545737.3924609063, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD: execution time=0.004 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma) # Copy-paste from the gradient descent section\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points\n",
    "#       and the model fit\n",
    "# ***************************************************\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840fbee6124f4f18aa7bd6814c87a258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions : \n",
    "- The outliers really influences the convergence of the this model, the optimal parameters are not the same in both situation (with and without outliers) \n",
    "- Without outliers : GD iter. 49/49: loss=545737.3924609063, w0=73.63227245973111, w1=14.475704876437092 - execution time=0.003 seconds\n",
    "- With the 2 outliers : GD iter. 49/49: loss=2690237.5111066285, w0=74.06780585492636, w1=11.0348948659891 - execution time=0.007 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_mae(y, tx, w):\n",
    "    \"\"\"Calculate the loss using MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    N = tx.shape[0]\n",
    "    e = y - tx@w \n",
    "    MAE = (1/2*N) * np.sum(np.abs(e))\n",
    "    return MAE\n",
    "    # TODO: compute loss by MAE\n",
    "    # ***************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # We need to use the chain rule as mentioned in the instructions - see the explication in the exercise (Ipad)\n",
    "    q_gradient = - tx\n",
    "    e=y-tx@w\n",
    "    dh=np.ones(len(y))\n",
    "    dh[e>0]=1\n",
    "    dh[e<0]=-1\n",
    "    dh[e==0]=0\n",
    "    subgrad_mae = dh@q_gradient/(2*len(y))\n",
    "    return subgrad_mae\n",
    "    # TODO: compute subgradient gradient vector for MAE\n",
    "    # ***************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        subgradient = compute_subgradient_mae(y, tx, w)\n",
    "        loss = compute_loss_mae(y, tx, w)\n",
    "        \n",
    "        # TODO: compute subgradient and loss\n",
    "        # ***************************************************\n",
    "        # ***************************************************\n",
    "        w = w - gamma*subgradient\n",
    "        \n",
    "        # TODO: update w by subgradient\n",
    "        # ***************************************************\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=1472645.4491946227, w0=0.35, w1=8.447686994372815e-16\n",
      "SubGD iter. 1/499: loss=1465645.449194623, w0=0.7, w1=1.689537398874563e-15\n",
      "SubGD iter. 2/499: loss=1458645.4491946227, w0=1.0499999999999998, w1=2.5343060983118446e-15\n",
      "SubGD iter. 3/499: loss=1451645.449194623, w0=1.4, w1=3.379074797749126e-15\n",
      "SubGD iter. 4/499: loss=1444645.449194623, w0=1.75, w1=4.2238434971864075e-15\n",
      "SubGD iter. 5/499: loss=1437645.449194623, w0=2.1, w1=5.068612196623689e-15\n",
      "SubGD iter. 6/499: loss=1430645.449194623, w0=2.45, w1=5.913380896060971e-15\n",
      "SubGD iter. 7/499: loss=1423645.4491946227, w0=2.8000000000000003, w1=6.758149595498253e-15\n",
      "SubGD iter. 8/499: loss=1416645.449194623, w0=3.1500000000000004, w1=7.602918294935534e-15\n",
      "SubGD iter. 9/499: loss=1409645.449194623, w0=3.5000000000000004, w1=8.447686994372815e-15\n",
      "SubGD iter. 10/499: loss=1402645.4491946227, w0=3.8500000000000005, w1=9.292455693810096e-15\n",
      "SubGD iter. 11/499: loss=1395645.449194623, w0=4.2, w1=1.0137224393247377e-14\n",
      "SubGD iter. 12/499: loss=1388645.449194623, w0=4.55, w1=1.0981993092684658e-14\n",
      "SubGD iter. 13/499: loss=1381645.4491946227, w0=4.8999999999999995, w1=1.1826761792121939e-14\n",
      "SubGD iter. 14/499: loss=1374645.449194623, w0=5.249999999999999, w1=1.267153049155922e-14\n",
      "SubGD iter. 15/499: loss=1367645.449194623, w0=5.599999999999999, w1=1.3516299190996501e-14\n",
      "SubGD iter. 16/499: loss=1360645.449194623, w0=5.949999999999998, w1=1.4361067890433782e-14\n",
      "SubGD iter. 17/499: loss=1353645.4491946232, w0=6.299999999999998, w1=1.5205836589871065e-14\n",
      "SubGD iter. 18/499: loss=1346645.4491946232, w0=6.649999999999998, w1=1.6050605289308347e-14\n",
      "SubGD iter. 19/499: loss=1339645.4491946232, w0=6.999999999999997, w1=1.689537398874563e-14\n",
      "SubGD iter. 20/499: loss=1332645.4491946232, w0=7.349999999999997, w1=1.7740142688182912e-14\n",
      "SubGD iter. 21/499: loss=1325645.449194623, w0=7.699999999999997, w1=1.8584911387620195e-14\n",
      "SubGD iter. 22/499: loss=1318645.4491946232, w0=8.049999999999997, w1=1.9429680087057478e-14\n",
      "SubGD iter. 23/499: loss=1311645.4491946232, w0=8.399999999999997, w1=2.027444878649476e-14\n",
      "SubGD iter. 24/499: loss=1304645.4491946232, w0=8.749999999999996, w1=2.1119217485932043e-14\n",
      "SubGD iter. 25/499: loss=1297645.4491946232, w0=9.099999999999996, w1=2.1963986185369325e-14\n",
      "SubGD iter. 26/499: loss=1290645.449194623, w0=9.449999999999996, w1=2.2808754884806608e-14\n",
      "SubGD iter. 27/499: loss=1283645.4491946232, w0=9.799999999999995, w1=2.365352358424389e-14\n",
      "SubGD iter. 28/499: loss=1276645.4491946232, w0=10.149999999999995, w1=2.4498292283681173e-14\n",
      "SubGD iter. 29/499: loss=1269645.4491946232, w0=10.499999999999995, w1=2.5343060983118456e-14\n",
      "SubGD iter. 30/499: loss=1262645.4491946232, w0=10.849999999999994, w1=2.6187829682555738e-14\n",
      "SubGD iter. 31/499: loss=1255645.449194623, w0=11.199999999999994, w1=2.703259838199302e-14\n",
      "SubGD iter. 32/499: loss=1248645.449194623, w0=11.549999999999994, w1=2.7877367081430304e-14\n",
      "SubGD iter. 33/499: loss=1241645.4491946232, w0=11.899999999999993, w1=2.872213578086758e-14\n",
      "SubGD iter. 34/499: loss=1234645.4491946232, w0=12.249999999999993, w1=2.956690448030486e-14\n",
      "SubGD iter. 35/499: loss=1227645.4491946232, w0=12.599999999999993, w1=3.041167317974214e-14\n",
      "SubGD iter. 36/499: loss=1220645.4491946232, w0=12.949999999999992, w1=3.125644187917942e-14\n",
      "SubGD iter. 37/499: loss=1213645.4491946232, w0=13.299999999999992, w1=3.21012105786167e-14\n",
      "SubGD iter. 38/499: loss=1206645.4491946232, w0=13.649999999999991, w1=3.294597927805398e-14\n",
      "SubGD iter. 39/499: loss=1199645.4491946232, w0=13.999999999999991, w1=3.379074797749126e-14\n",
      "SubGD iter. 40/499: loss=1192645.4491946232, w0=14.34999999999999, w1=3.463551667692854e-14\n",
      "SubGD iter. 41/499: loss=1185645.4491946232, w0=14.69999999999999, w1=3.548028537636582e-14\n",
      "SubGD iter. 42/499: loss=1178645.4491946232, w0=15.04999999999999, w1=3.63250540758031e-14\n",
      "SubGD iter. 43/499: loss=1171645.4491946232, w0=15.39999999999999, w1=3.716982277524038e-14\n",
      "SubGD iter. 44/499: loss=1164645.4491946232, w0=15.74999999999999, w1=3.801459147467766e-14\n",
      "SubGD iter. 45/499: loss=1157645.4491946234, w0=16.09999999999999, w1=3.8859360174114936e-14\n",
      "SubGD iter. 46/499: loss=1150645.4491946232, w0=16.449999999999992, w1=3.9704128873552216e-14\n",
      "SubGD iter. 47/499: loss=1143645.4491946232, w0=16.799999999999994, w1=4.0548897572989495e-14\n",
      "SubGD iter. 48/499: loss=1136645.4491946232, w0=17.149999999999995, w1=4.1393666272426775e-14\n",
      "SubGD iter. 49/499: loss=1129645.449194623, w0=17.499999999999996, w1=4.2238434971864054e-14\n",
      "SubGD iter. 50/499: loss=1122645.449194623, w0=17.849999999999998, w1=4.3083203671301334e-14\n",
      "SubGD iter. 51/499: loss=1115645.4491946232, w0=18.2, w1=4.392797237073861e-14\n",
      "SubGD iter. 52/499: loss=1108645.4491946232, w0=18.55, w1=4.477274107017589e-14\n",
      "SubGD iter. 53/499: loss=1101645.4491946232, w0=18.900000000000002, w1=4.561750976961317e-14\n",
      "SubGD iter. 54/499: loss=1094645.4491946227, w0=19.250000000000004, w1=4.646227846905045e-14\n",
      "SubGD iter. 55/499: loss=1087645.4491946227, w0=19.600000000000005, w1=4.730704716848773e-14\n",
      "SubGD iter. 56/499: loss=1080645.4491946227, w0=19.950000000000006, w1=4.815181586792501e-14\n",
      "SubGD iter. 57/499: loss=1073645.449194623, w0=20.300000000000008, w1=4.899658456736229e-14\n",
      "SubGD iter. 58/499: loss=1066645.4491946227, w0=20.65000000000001, w1=4.984135326679957e-14\n",
      "SubGD iter. 59/499: loss=1059645.4491946227, w0=21.00000000000001, w1=5.068612196623685e-14\n",
      "SubGD iter. 60/499: loss=1052645.4491946227, w0=21.350000000000012, w1=5.153089066567413e-14\n",
      "SubGD iter. 61/499: loss=1045645.4491946228, w0=21.700000000000014, w1=5.237565936511141e-14\n",
      "SubGD iter. 62/499: loss=1038645.4491946228, w0=22.050000000000015, w1=5.3220428064548687e-14\n",
      "SubGD iter. 63/499: loss=1031645.4491946228, w0=22.400000000000016, w1=5.4065196763985966e-14\n",
      "SubGD iter. 64/499: loss=1024645.4491946228, w0=22.750000000000018, w1=5.4909965463423246e-14\n",
      "SubGD iter. 65/499: loss=1017645.4491946226, w0=23.10000000000002, w1=5.5754734162860525e-14\n",
      "SubGD iter. 66/499: loss=1010645.4491946226, w0=23.45000000000002, w1=5.6599502862297805e-14\n",
      "SubGD iter. 67/499: loss=1003645.4491946225, w0=23.800000000000022, w1=5.744427156173509e-14\n",
      "SubGD iter. 68/499: loss=996645.4491946226, w0=24.150000000000023, w1=5.828904026117238e-14\n",
      "SubGD iter. 69/499: loss=989645.4491946225, w0=24.500000000000025, w1=5.913380896060966e-14\n",
      "SubGD iter. 70/499: loss=982645.4491946225, w0=24.850000000000026, w1=5.997857766004695e-14\n",
      "SubGD iter. 71/499: loss=975645.4491946225, w0=25.200000000000028, w1=6.082334635948423e-14\n",
      "SubGD iter. 72/499: loss=968645.4491946225, w0=25.55000000000003, w1=6.166811505892152e-14\n",
      "SubGD iter. 73/499: loss=961645.4491946225, w0=25.90000000000003, w1=6.25128837583588e-14\n",
      "SubGD iter. 74/499: loss=954645.4491946225, w0=26.250000000000032, w1=6.335765245779609e-14\n",
      "SubGD iter. 75/499: loss=947645.4491946225, w0=26.600000000000033, w1=6.420242115723338e-14\n",
      "SubGD iter. 76/499: loss=940645.4491946222, w0=26.950000000000035, w1=6.504718985667066e-14\n",
      "SubGD iter. 77/499: loss=933645.4491946222, w0=27.300000000000036, w1=6.589195855610795e-14\n",
      "SubGD iter. 78/499: loss=926645.4491946222, w0=27.650000000000038, w1=6.673672725554523e-14\n",
      "SubGD iter. 79/499: loss=919645.4491946222, w0=28.00000000000004, w1=6.758149595498252e-14\n",
      "SubGD iter. 80/499: loss=912645.4491946222, w0=28.35000000000004, w1=6.84262646544198e-14\n",
      "SubGD iter. 81/499: loss=905645.449194622, w0=28.700000000000042, w1=6.927103335385709e-14\n",
      "SubGD iter. 82/499: loss=898645.449194622, w0=29.050000000000043, w1=7.011580205329438e-14\n",
      "SubGD iter. 83/499: loss=891645.449194622, w0=29.400000000000045, w1=7.096057075273166e-14\n",
      "SubGD iter. 84/499: loss=884645.449194622, w0=29.750000000000046, w1=7.180533945216895e-14\n",
      "SubGD iter. 85/499: loss=877645.449194622, w0=30.100000000000048, w1=7.265010815160623e-14\n",
      "SubGD iter. 86/499: loss=870645.449194622, w0=30.45000000000005, w1=7.349487685104352e-14\n",
      "SubGD iter. 87/499: loss=863645.449194622, w0=30.80000000000005, w1=7.43396455504808e-14\n",
      "SubGD iter. 88/499: loss=856645.4491946219, w0=31.150000000000052, w1=7.518441424991809e-14\n",
      "SubGD iter. 89/499: loss=849645.4491946219, w0=31.500000000000053, w1=7.602918294935538e-14\n",
      "SubGD iter. 90/499: loss=842645.4491946219, w0=31.850000000000055, w1=7.687395164879266e-14\n",
      "SubGD iter. 91/499: loss=835645.4491946219, w0=32.20000000000005, w1=7.771872034822995e-14\n",
      "SubGD iter. 92/499: loss=828645.4491946219, w0=32.550000000000054, w1=7.856348904766723e-14\n",
      "SubGD iter. 93/499: loss=821645.4491946219, w0=32.900000000000055, w1=7.940825774710452e-14\n",
      "SubGD iter. 94/499: loss=814645.4491946219, w0=33.25000000000006, w1=8.02530264465418e-14\n",
      "SubGD iter. 95/499: loss=807645.4491946219, w0=33.60000000000006, w1=8.109779514597909e-14\n",
      "SubGD iter. 96/499: loss=800645.4491946218, w0=33.95000000000006, w1=8.194256384541638e-14\n",
      "SubGD iter. 97/499: loss=793645.4491946216, w0=34.30000000000006, w1=8.278733254485366e-14\n",
      "SubGD iter. 98/499: loss=786645.4491946218, w0=34.65000000000006, w1=8.363210124429095e-14\n",
      "SubGD iter. 99/499: loss=779645.4491946218, w0=35.000000000000064, w1=8.447686994372823e-14\n",
      "SubGD iter. 100/499: loss=772645.4491946216, w0=35.350000000000065, w1=8.532163864316552e-14\n",
      "SubGD iter. 101/499: loss=765645.4491946216, w0=35.70000000000007, w1=8.61664073426028e-14\n",
      "SubGD iter. 102/499: loss=758645.4491946216, w0=36.05000000000007, w1=8.701117604204009e-14\n",
      "SubGD iter. 103/499: loss=751645.4491946215, w0=36.40000000000007, w1=8.785594474147738e-14\n",
      "SubGD iter. 104/499: loss=744645.4491946215, w0=36.75000000000007, w1=8.870071344091466e-14\n",
      "SubGD iter. 105/499: loss=737645.4491946215, w0=37.10000000000007, w1=8.954548214035195e-14\n",
      "SubGD iter. 106/499: loss=730645.4491946216, w0=37.450000000000074, w1=9.039025083978923e-14\n",
      "SubGD iter. 107/499: loss=723645.4491946215, w0=37.800000000000075, w1=9.123501953922652e-14\n",
      "SubGD iter. 108/499: loss=716645.4491946215, w0=38.15000000000008, w1=9.20797882386638e-14\n",
      "SubGD iter. 109/499: loss=709645.4491946215, w0=38.50000000000008, w1=9.292455693810109e-14\n",
      "SubGD iter. 110/499: loss=702645.4491946214, w0=38.85000000000008, w1=9.376932563753838e-14\n",
      "SubGD iter. 111/499: loss=695645.4491946213, w0=39.20000000000008, w1=9.461409433697566e-14\n",
      "SubGD iter. 112/499: loss=688645.4491946213, w0=39.55000000000008, w1=9.545886303641295e-14\n",
      "SubGD iter. 113/499: loss=681645.4491946213, w0=39.900000000000084, w1=9.630363173585023e-14\n",
      "SubGD iter. 114/499: loss=674645.4491946212, w0=40.250000000000085, w1=9.714840043528752e-14\n",
      "SubGD iter. 115/499: loss=667645.4491946212, w0=40.60000000000009, w1=9.799316913472481e-14\n",
      "SubGD iter. 116/499: loss=660645.4491946212, w0=40.95000000000009, w1=9.883793783416209e-14\n",
      "SubGD iter. 117/499: loss=653645.4491946213, w0=41.30000000000009, w1=9.968270653359938e-14\n",
      "SubGD iter. 118/499: loss=646645.4491946212, w0=41.65000000000009, w1=1.0052747523303666e-13\n",
      "SubGD iter. 119/499: loss=639645.4491946212, w0=42.00000000000009, w1=1.0137224393247395e-13\n",
      "SubGD iter. 120/499: loss=632645.4491946212, w0=42.350000000000094, w1=1.0221701263191124e-13\n",
      "SubGD iter. 121/499: loss=625645.4491946212, w0=42.700000000000095, w1=1.0306178133134852e-13\n",
      "SubGD iter. 122/499: loss=618645.4491946211, w0=43.0500000000001, w1=1.0390655003078581e-13\n",
      "SubGD iter. 123/499: loss=611645.449194621, w0=43.4000000000001, w1=1.0475131873022309e-13\n",
      "SubGD iter. 124/499: loss=604645.449194621, w0=43.7500000000001, w1=1.0559608742966038e-13\n",
      "SubGD iter. 125/499: loss=597645.449194621, w0=44.1000000000001, w1=1.0644085612909766e-13\n",
      "SubGD iter. 126/499: loss=590645.449194621, w0=44.4500000000001, w1=1.0728562482853495e-13\n",
      "SubGD iter. 127/499: loss=583645.449194621, w0=44.800000000000104, w1=1.0813039352797224e-13\n",
      "SubGD iter. 128/499: loss=576645.449194621, w0=45.150000000000105, w1=1.0897516222740952e-13\n",
      "SubGD iter. 129/499: loss=569645.4491946208, w0=45.50000000000011, w1=1.0981993092684681e-13\n",
      "SubGD iter. 130/499: loss=562645.4491946208, w0=45.85000000000011, w1=1.1066469962628409e-13\n",
      "SubGD iter. 131/499: loss=555645.4491946208, w0=46.20000000000011, w1=1.1150946832572138e-13\n",
      "SubGD iter. 132/499: loss=548645.4491946208, w0=46.55000000000011, w1=1.1235423702515866e-13\n",
      "SubGD iter. 133/499: loss=541685.8291604209, w0=46.89650000000011, w1=0.006411364611476446\n",
      "SubGD iter. 134/499: loss=534822.7802692107, w0=47.24300000000011, w1=0.012822729222840537\n",
      "SubGD iter. 135/499: loss=527960.6733095637, w0=47.58600000000011, w1=0.025445147149862915\n",
      "SubGD iter. 136/499: loss=521228.76899903076, w0=47.929000000000116, w1=0.03806756507688529\n",
      "SubGD iter. 137/499: loss=514496.86468849785, w0=48.27200000000012, w1=0.05068998300390766\n",
      "SubGD iter. 138/499: loss=507764.96037796495, w0=48.61500000000012, w1=0.06331240093093003\n",
      "SubGD iter. 139/499: loss=501067.60244489525, w0=48.95100000000012, w1=0.08755299518327002\n",
      "SubGD iter. 140/499: loss=494607.86996113986, w0=49.283500000000124, w1=0.11659806010203295\n",
      "SubGD iter. 141/499: loss=488266.442741708, w0=49.612500000000125, w1=0.15237637325784545\n",
      "SubGD iter. 142/499: loss=482008.094873578, w0=49.941500000000126, w1=0.18815468641365796\n",
      "SubGD iter. 143/499: loss=475749.747005448, w0=50.270500000000126, w1=0.22393299956947046\n",
      "SubGD iter. 144/499: loss=469491.39913731796, w0=50.59950000000013, w1=0.259711312725283\n",
      "SubGD iter. 145/499: loss=463253.8552405502, w0=50.92150000000013, w1=0.30747197293455375\n",
      "SubGD iter. 146/499: loss=457198.7077740573, w0=51.24350000000013, w1=0.3552326331438245\n",
      "SubGD iter. 147/499: loss=451143.56030756445, w0=51.565500000000135, w1=0.4029932933530953\n",
      "SubGD iter. 148/499: loss=445129.7573161638, w0=51.884000000000135, w1=0.4555882439014485\n",
      "SubGD iter. 149/499: loss=439216.91395101754, w0=52.19900000000013, w1=0.5130097747510801\n",
      "SubGD iter. 150/499: loss=433358.50068215386, w0=52.51400000000013, w1=0.5704313056007118\n",
      "SubGD iter. 151/499: loss=427529.51052556746, w0=52.82550000000013, w1=0.6332530053453437\n",
      "SubGD iter. 152/499: loss=421769.7280539167, w0=53.13350000000013, w1=0.7007243854921134\n",
      "SubGD iter. 153/499: loss=416103.91235559876, w0=53.43800000000013, w1=0.7729038954261416\n",
      "SubGD iter. 154/499: loss=410507.904832495, w0=53.74250000000013, w1=0.8450834053601699\n",
      "SubGD iter. 155/499: loss=404930.9865247631, w0=54.04350000000013, w1=0.9224309733757141\n",
      "SubGD iter. 156/499: loss=399441.8762701324, w0=54.34100000000013, w1=1.0034009434534867\n",
      "SubGD iter. 157/499: loss=394009.7399241668, w0=54.63850000000013, w1=1.0843709135312594\n",
      "SubGD iter. 158/499: loss=388577.60357820144, w0=54.93600000000013, w1=1.165340883609032\n",
      "SubGD iter. 159/499: loss=383150.45574367937, w0=55.230000000000125, w1=1.2505503517505483\n",
      "SubGD iter. 160/499: loss=377796.36126019596, w0=55.52400000000012, w1=1.3357598198920646\n",
      "SubGD iter. 161/499: loss=372479.8742824207, w0=55.81100000000012, w1=1.4278592279171984\n",
      "SubGD iter. 162/499: loss=367288.3713705018, w0=56.09800000000012, w1=1.5199586359423323\n",
      "SubGD iter. 163/499: loss=362124.2451844805, w0=56.381500000000116, w1=1.6146217580388682\n",
      "SubGD iter. 164/499: loss=357065.5796263361, w0=56.65450000000012, w1=1.7217064493776948\n",
      "SubGD iter. 165/499: loss=352183.0355041871, w0=56.92400000000012, w1=1.832055114301137\n",
      "SubGD iter. 166/499: loss=347369.16344326956, w0=57.19000000000012, w1=1.9453926924248666\n",
      "SubGD iter. 167/499: loss=342591.9402081294, w0=57.45600000000012, w1=2.058730270548596\n",
      "SubGD iter. 168/499: loss=337814.7169729893, w0=57.722000000000115, w1=2.1720678486723255\n",
      "SubGD iter. 169/499: loss=333039.02952206845, w0=57.98450000000012, w1=2.286240259905451\n",
      "SubGD iter. 170/499: loss=328393.2025092348, w0=58.23650000000012, w1=2.4103482575821307\n",
      "SubGD iter. 171/499: loss=323892.85794423387, w0=58.48500000000012, w1=2.538973562055642\n",
      "SubGD iter. 172/499: loss=319418.7597184679, w0=58.73350000000012, w1=2.6675988665291532\n",
      "SubGD iter. 173/499: loss=314955.9287076171, w0=58.97850000000012, w1=2.7978992657399004\n",
      "SubGD iter. 174/499: loss=310573.2147446256, w0=59.22000000000012, w1=2.92995066792203\n",
      "SubGD iter. 175/499: loss=306244.0820121532, w0=59.46150000000012, w1=3.0620020701041595\n",
      "SubGD iter. 176/499: loss=301914.94927968085, w0=59.703000000000124, w1=3.194053472286289\n",
      "SubGD iter. 177/499: loss=297598.57127249695, w0=59.94100000000012, w1=3.3263513871948547\n",
      "SubGD iter. 178/499: loss=293361.61479883105, w0=60.17900000000012, w1=3.4586493021034204\n",
      "SubGD iter. 179/499: loss=289124.65832516516, w0=60.41700000000012, w1=3.590947217011986\n",
      "SubGD iter. 180/499: loss=284894.46163547644, w0=60.65150000000012, w1=3.7261297383444725\n",
      "SubGD iter. 181/499: loss=280779.3950624084, w0=60.86850000000012, w1=3.876026704379692\n",
      "SubGD iter. 182/499: loss=276813.21198208135, w0=61.08200000000012, w1=4.028893275039291\n",
      "SubGD iter. 183/499: loss=272873.18692921125, w0=61.295500000000125, w1=4.18175984569889\n",
      "SubGD iter. 184/499: loss=268936.50696741394, w0=61.505500000000126, w1=4.33693233191822\n",
      "SubGD iter. 185/499: loss=265040.5926543004, w0=61.71550000000013, w1=4.49210481813755\n",
      "SubGD iter. 186/499: loss=261144.67834118678, w0=61.92550000000013, w1=4.64727730435688\n",
      "SubGD iter. 187/499: loss=257248.7640280731, w0=62.13550000000013, w1=4.80244979057621\n",
      "SubGD iter. 188/499: loss=253352.84971495956, w0=62.34550000000013, w1=4.95762227679554\n",
      "SubGD iter. 189/499: loss=249468.3722827554, w0=62.55200000000013, w1=5.112652903119136\n",
      "SubGD iter. 190/499: loss=245668.52226338635, w0=62.75150000000013, w1=5.273578519533768\n",
      "SubGD iter. 191/499: loss=241927.5906218524, w0=62.947500000000126, w1=5.435878668999408\n",
      "SubGD iter. 192/499: loss=238227.17127804845, w0=63.143500000000124, w1=5.598178818465048\n",
      "SubGD iter. 193/499: loss=234526.75193424453, w0=63.33950000000012, w1=5.760478967930688\n",
      "SubGD iter. 194/499: loss=230826.33259044064, w0=63.53550000000012, w1=5.922779117396328\n",
      "SubGD iter. 195/499: loss=227143.4395582236, w0=63.72450000000012, w1=6.0906867856343485\n",
      "SubGD iter. 196/499: loss=223491.2118409019, w0=63.91350000000012, w1=6.2585944538723695\n",
      "SubGD iter. 197/499: loss=219856.97793258174, w0=64.10250000000012, w1=6.423992525077158\n",
      "SubGD iter. 198/499: loss=216252.5481063952, w0=64.29150000000011, w1=6.589390596281946\n",
      "SubGD iter. 199/499: loss=212649.64358952717, w0=64.48400000000011, w1=6.749203965000739\n",
      "SubGD iter. 200/499: loss=209072.69714259868, w0=64.6765000000001, w1=6.909017333719532\n",
      "SubGD iter. 201/499: loss=205500.9557497483, w0=64.8655000000001, w1=7.0703536218939\n",
      "SubGD iter. 202/499: loss=201986.1131688051, w0=65.0510000000001, w1=7.233426353251043\n",
      "SubGD iter. 203/499: loss=198500.22941381764, w0=65.2365000000001, w1=7.396499084608187\n",
      "SubGD iter. 204/499: loss=195014.34565883022, w0=65.42200000000011, w1=7.55957181596533\n",
      "SubGD iter. 205/499: loss=191528.46190384284, w0=65.60750000000012, w1=7.722644547322473\n",
      "SubGD iter. 206/499: loss=188050.23786594527, w0=65.78950000000012, w1=7.886027772424261\n",
      "SubGD iter. 207/499: loss=184632.06196625027, w0=65.97150000000012, w1=8.04941099752605\n",
      "SubGD iter. 208/499: loss=181241.81307053572, w0=66.15350000000012, w1=8.206123162955011\n",
      "SubGD iter. 209/499: loss=177945.6586251966, w0=66.33550000000012, w1=8.362835328383973\n",
      "SubGD iter. 210/499: loss=174674.52147647995, w0=66.52100000000013, w1=8.512662251384173\n",
      "SubGD iter. 211/499: loss=171445.39390000526, w0=66.70300000000013, w1=8.663667521441432\n",
      "SubGD iter. 212/499: loss=168249.58866657288, w0=66.88500000000013, w1=8.814672791498692\n",
      "SubGD iter. 213/499: loss=165053.78343314052, w0=67.06700000000014, w1=8.965678061555952\n",
      "SubGD iter. 214/499: loss=161869.96885881646, w0=67.24550000000013, w1=9.11540845822547\n",
      "SubGD iter. 215/499: loss=158768.17219099868, w0=67.42400000000013, w1=9.265138854894989\n",
      "SubGD iter. 216/499: loss=155674.98182593545, w0=67.59550000000013, w1=9.418069673306734\n",
      "SubGD iter. 217/499: loss=152678.44728047712, w0=67.76350000000014, w1=9.567814979903314\n",
      "SubGD iter. 218/499: loss=149784.29546060823, w0=67.93150000000014, w1=9.717560286499895\n",
      "SubGD iter. 219/499: loss=146904.91819735558, w0=68.09600000000015, w1=9.86637411905652\n",
      "SubGD iter. 220/499: loss=144138.20482989826, w0=68.25700000000015, w1=10.012318441477936\n",
      "SubGD iter. 221/499: loss=141449.8893661133, w0=68.41450000000015, w1=10.16041037764196\n",
      "SubGD iter. 222/499: loss=138793.12482711006, w0=68.57550000000015, w1=10.301769065568932\n",
      "SubGD iter. 223/499: loss=136201.12387641103, w0=68.72600000000014, w1=10.445535111729411\n",
      "SubGD iter. 224/499: loss=133725.90466557408, w0=68.87300000000015, w1=10.59248139130233\n",
      "SubGD iter. 225/499: loss=131264.82859226177, w0=69.01650000000015, w1=10.741958091010359\n",
      "SubGD iter. 226/499: loss=128855.44330581016, w0=69.15650000000015, w1=10.889257348165403\n",
      "SubGD iter. 227/499: loss=126525.85020630735, w0=69.29300000000015, w1=11.031415883375482\n",
      "SubGD iter. 228/499: loss=124311.80409482561, w0=69.43300000000015, w1=11.169057111788728\n",
      "SubGD iter. 229/499: loss=122130.81279499263, w0=69.56950000000015, w1=11.30161982693959\n",
      "SubGD iter. 230/499: loss=120061.94859795446, w0=69.70600000000015, w1=11.43418254209045\n",
      "SubGD iter. 231/499: loss=117993.08440091627, w0=69.84250000000014, w1=11.566745257241312\n",
      "SubGD iter. 232/499: loss=115924.22020387813, w0=69.97900000000014, w1=11.699307972392173\n",
      "SubGD iter. 233/499: loss=113855.35600683992, w0=70.11550000000014, w1=11.831870687543034\n",
      "SubGD iter. 234/499: loss=111820.10503842625, w0=70.24500000000013, w1=11.962151953406378\n",
      "SubGD iter. 235/499: loss=109891.90742500029, w0=70.37450000000013, w1=12.092433219269722\n",
      "SubGD iter. 236/499: loss=107979.36273081512, w0=70.50050000000013, w1=12.223997516498684\n",
      "SubGD iter. 237/499: loss=106083.06762765218, w0=70.62650000000014, w1=12.355561813727647\n",
      "SubGD iter. 238/499: loss=104211.02725167475, w0=70.75250000000014, w1=12.47565220598762\n",
      "SubGD iter. 239/499: loss=102508.307304134, w0=70.87500000000014, w1=12.593944930852317\n",
      "SubGD iter. 240/499: loss=100859.80336510134, w0=71.00100000000015, w1=12.706837486822012\n",
      "SubGD iter. 241/499: loss=99260.88189926598, w0=71.12000000000015, w1=12.82015432605579\n",
      "SubGD iter. 242/499: loss=97747.37645628284, w0=71.23550000000014, w1=12.9308941520685\n",
      "SubGD iter. 243/499: loss=96318.45860853119, w0=71.34750000000014, w1=13.03791893418546\n",
      "SubGD iter. 244/499: loss=94973.37984850902, w0=71.45600000000015, w1=13.13925830040779\n",
      "SubGD iter. 245/499: loss=93757.14539712733, w0=71.56100000000015, w1=13.234145438723894\n",
      "SubGD iter. 246/499: loss=92612.65573896616, w0=71.66600000000015, w1=13.32903257704\n",
      "SubGD iter. 247/499: loss=91468.16608080501, w0=71.77100000000016, w1=13.423919715356105\n",
      "SubGD iter. 248/499: loss=90407.80130215929, w0=71.85850000000016, w1=13.5020497211445\n",
      "SubGD iter. 249/499: loss=89646.89748226527, w0=71.93900000000016, w1=13.567949035141499\n",
      "SubGD iter. 250/499: loss=89091.94125838435, w0=72.00900000000016, w1=13.622762127258913\n",
      "SubGD iter. 251/499: loss=88640.25696881454, w0=72.07900000000015, w1=13.677575219376326\n",
      "SubGD iter. 252/499: loss=88188.57267924475, w0=72.14900000000014, w1=13.73238831149374\n",
      "SubGD iter. 253/499: loss=87744.69994792514, w0=72.20850000000014, w1=13.79076641121072\n",
      "SubGD iter. 254/499: loss=87381.2664355986, w0=72.26100000000014, w1=13.843386351310635\n",
      "SubGD iter. 255/499: loss=87065.54597296327, w0=72.31350000000013, w1=13.896006291410549\n",
      "SubGD iter. 256/499: loss=86752.90972175734, w0=72.36250000000014, w1=13.948063022843124\n",
      "SubGD iter. 257/499: loss=86462.18697865402, w0=72.40800000000014, w1=13.998690867055366\n",
      "SubGD iter. 258/499: loss=86200.3897930706, w0=72.44650000000014, w1=14.041454085071832\n",
      "SubGD iter. 259/499: loss=86011.19306077783, w0=72.48500000000014, w1=14.084217303088298\n",
      "SubGD iter. 260/499: loss=85821.99632848504, w0=72.52350000000014, w1=14.126980521104764\n",
      "SubGD iter. 261/499: loss=85632.79959619226, w0=72.56200000000014, w1=14.16974373912123\n",
      "SubGD iter. 262/499: loss=85454.0497808494, w0=72.59700000000014, w1=14.209245337915945\n",
      "SubGD iter. 263/499: loss=85308.754306304, w0=72.62500000000014, w1=14.238368242808056\n",
      "SubGD iter. 264/499: loss=85215.48895834087, w0=72.65300000000015, w1=14.267491147700166\n",
      "SubGD iter. 265/499: loss=85123.33733137815, w0=72.67750000000015, w1=14.292564081356748\n",
      "SubGD iter. 266/499: loss=85053.11435982685, w0=72.70200000000015, w1=14.317637015013329\n",
      "SubGD iter. 267/499: loss=84982.89138827559, w0=72.72650000000016, w1=14.34270994866991\n",
      "SubGD iter. 268/499: loss=84912.6684167243, w0=72.75100000000016, w1=14.367782882326491\n",
      "SubGD iter. 269/499: loss=84842.44544517301, w0=72.77550000000016, w1=14.392855815983072\n",
      "SubGD iter. 270/499: loss=84772.22247362175, w0=72.80000000000017, w1=14.417928749639653\n",
      "SubGD iter. 271/499: loss=84702.27937110823, w0=72.82800000000017, w1=14.436790629980576\n",
      "SubGD iter. 272/499: loss=84637.14962653705, w0=72.85600000000018, w1=14.4556525103215\n",
      "SubGD iter. 273/499: loss=84572.01988196588, w0=72.88400000000019, w1=14.474514390662423\n",
      "SubGD iter. 274/499: loss=84508.6641002645, w0=72.90850000000019, w1=14.491059599354255\n",
      "SubGD iter. 275/499: loss=84458.96232066532, w0=72.92950000000019, w1=14.504848036239448\n",
      "SubGD iter. 276/499: loss=84422.89826399462, w0=72.95050000000019, w1=14.51863647312464\n",
      "SubGD iter. 277/499: loss=84386.83420732396, w0=72.97150000000019, w1=14.532424910009833\n",
      "SubGD iter. 278/499: loss=84350.77015065326, w0=72.99250000000019, w1=14.546213346895026\n",
      "SubGD iter. 279/499: loss=84315.03055570369, w0=73.0170000000002, w1=14.553590419168856\n",
      "SubGD iter. 280/499: loss=84286.21111829512, w0=73.03100000000019, w1=14.562430535668945\n",
      "SubGD iter. 281/499: loss=84270.54553773884, w0=73.04500000000019, w1=14.571270652169034\n",
      "SubGD iter. 282/499: loss=84254.87995718257, w0=73.05900000000018, w1=14.580110768669122\n",
      "SubGD iter. 283/499: loss=84239.21437662626, w0=73.07300000000018, w1=14.588950885169211\n",
      "SubGD iter. 284/499: loss=84223.54879606997, w0=73.08700000000017, w1=14.5977910016693\n",
      "SubGD iter. 285/499: loss=84208.20385726664, w0=73.10450000000017, w1=14.600219753558026\n",
      "SubGD iter. 286/499: loss=84192.65633267675, w0=73.11850000000017, w1=14.609059870058115\n",
      "SubGD iter. 287/499: loss=84176.99075212047, w0=73.13250000000016, w1=14.617899986558204\n",
      "SubGD iter. 288/499: loss=84161.32517156418, w0=73.14650000000016, w1=14.626740103058292\n",
      "SubGD iter. 289/499: loss=84149.98201000808, w0=73.15700000000015, w1=14.632347529408372\n",
      "SubGD iter. 290/499: loss=84141.88525399259, w0=73.16750000000015, w1=14.637954955758453\n",
      "SubGD iter. 291/499: loss=84133.78849797705, w0=73.17800000000014, w1=14.643562382108533\n",
      "SubGD iter. 292/499: loss=84125.69174196155, w0=73.18850000000013, w1=14.649169808458613\n",
      "SubGD iter. 293/499: loss=84117.59498594605, w0=73.19900000000013, w1=14.654777234808693\n",
      "SubGD iter. 294/499: loss=84109.49822993053, w0=73.20950000000012, w1=14.660384661158773\n",
      "SubGD iter. 295/499: loss=84101.401473915, w0=73.22000000000011, w1=14.665992087508853\n",
      "SubGD iter. 296/499: loss=84093.3047178995, w0=73.2305000000001, w1=14.671599513858933\n",
      "SubGD iter. 297/499: loss=84085.20796188398, w0=73.2410000000001, w1=14.677206940209013\n",
      "SubGD iter. 298/499: loss=84077.11120586845, w0=73.25150000000009, w1=14.682814366559093\n",
      "SubGD iter. 299/499: loss=84069.01444985294, w0=73.26200000000009, w1=14.688421792909173\n",
      "SubGD iter. 300/499: loss=84060.91769383744, w0=73.27250000000008, w1=14.694029219259253\n",
      "SubGD iter. 301/499: loss=84052.82093782193, w0=73.28300000000007, w1=14.699636645609333\n",
      "SubGD iter. 302/499: loss=84044.72418180642, w0=73.29350000000007, w1=14.705244071959413\n",
      "SubGD iter. 303/499: loss=84040.79985258263, w0=73.29700000000007, w1=14.705707339464487\n",
      "SubGD iter. 304/499: loss=84040.08758876658, w0=73.30050000000007, w1=14.706170606969561\n",
      "SubGD iter. 305/499: loss=84039.3753249505, w0=73.30400000000007, w1=14.706633874474635\n",
      "SubGD iter. 306/499: loss=84039.10505406575, w0=73.30400000000007, w1=14.704331250979978\n",
      "SubGD iter. 307/499: loss=84038.80207835384, w0=73.30400000000007, w1=14.702028627485321\n",
      "SubGD iter. 308/499: loss=84038.78497320773, w0=73.30750000000008, w1=14.702491894990395\n",
      "SubGD iter. 309/499: loss=84038.7837972154, w0=73.30400000000007, w1=14.706400324811398\n",
      "SubGD iter. 310/499: loss=84039.07432395477, w0=73.30400000000007, w1=14.70409770131674\n",
      "SubGD iter. 311/499: loss=84038.7713482429, w0=73.30400000000007, w1=14.701795077822084\n",
      "SubGD iter. 312/499: loss=84038.7911558346, w0=73.30750000000008, w1=14.702258345327158\n",
      "SubGD iter. 313/499: loss=84038.83595792788, w0=73.30400000000007, w1=14.70616677514816\n",
      "SubGD iter. 314/499: loss=84039.04359384383, w0=73.30400000000007, w1=14.703864151653503\n",
      "SubGD iter. 315/499: loss=84038.74061813192, w0=73.30400000000007, w1=14.701561528158846\n",
      "SubGD iter. 316/499: loss=84038.79733846143, w0=73.30750000000008, w1=14.70202479566392\n",
      "SubGD iter. 317/499: loss=84038.88811864036, w0=73.30400000000007, w1=14.705933225484923\n",
      "SubGD iter. 318/499: loss=84039.01286373287, w0=73.30400000000007, w1=14.703630601990266\n",
      "SubGD iter. 319/499: loss=84038.74256505161, w0=73.30750000000008, w1=14.70409386949534\n",
      "SubGD iter. 320/499: loss=84038.77084405762, w0=73.30750000000008, w1=14.701791246000683\n",
      "SubGD iter. 321/499: loss=84038.94027935284, w0=73.30400000000007, w1=14.705699675821686\n",
      "SubGD iter. 322/499: loss=84038.98213362193, w0=73.30400000000007, w1=14.703397052327029\n",
      "SubGD iter. 323/499: loss=84038.74874767846, w0=73.30750000000008, w1=14.703860319832103\n",
      "SubGD iter. 324/499: loss=84038.74011394668, w0=73.30750000000008, w1=14.701557696337446\n",
      "SubGD iter. 325/499: loss=84038.99244006531, w0=73.30400000000007, w1=14.705466126158449\n",
      "SubGD iter. 326/499: loss=84038.95140351096, w0=73.30400000000007, w1=14.703163502663791\n",
      "SubGD iter. 327/499: loss=84038.7549303053, w0=73.30750000000008, w1=14.703626770168865\n",
      "SubGD iter. 328/499: loss=84038.70938383573, w0=73.30750000000008, w1=14.701324146674208\n",
      "SubGD iter. 329/499: loss=84039.04460077781, w0=73.30400000000007, w1=14.705232576495211\n",
      "SubGD iter. 330/499: loss=84038.9206734, w0=73.30400000000007, w1=14.702929953000554\n",
      "SubGD iter. 331/499: loss=84038.76111293214, w0=73.30750000000008, w1=14.703393220505628\n",
      "SubGD iter. 332/499: loss=84038.67865372477, w0=73.30750000000008, w1=14.701090597010971\n",
      "SubGD iter. 333/499: loss=84039.0967614903, w0=73.30400000000007, w1=14.704999026831974\n",
      "SubGD iter. 334/499: loss=84038.88994328905, w0=73.30400000000007, w1=14.702696403337317\n",
      "SubGD iter. 335/499: loss=84038.767295559, w0=73.30750000000008, w1=14.70315967084239\n",
      "SubGD iter. 336/499: loss=84038.6479236138, w0=73.30750000000008, w1=14.700857047347734\n",
      "SubGD iter. 337/499: loss=84039.14892220277, w0=73.30400000000007, w1=14.704765477168737\n",
      "SubGD iter. 338/499: loss=84038.85921317809, w0=73.30400000000007, w1=14.70246285367408\n",
      "SubGD iter. 339/499: loss=84038.77347818585, w0=73.30750000000008, w1=14.702926121179154\n",
      "SubGD iter. 340/499: loss=84038.6868176391, w0=73.30400000000007, w1=14.706834551000156\n",
      "SubGD iter. 341/499: loss=84039.13145877903, w0=73.30400000000007, w1=14.7045319275055\n",
      "SubGD iter. 342/499: loss=84038.82848306713, w0=73.30400000000007, w1=14.702229304010842\n",
      "SubGD iter. 343/499: loss=84038.77966081268, w0=73.30750000000008, w1=14.702692571515916\n",
      "SubGD iter. 344/499: loss=84038.73897835157, w0=73.30400000000007, w1=14.706601001336919\n",
      "SubGD iter. 345/499: loss=84039.10072866808, w0=73.30400000000007, w1=14.704298377842262\n",
      "SubGD iter. 346/499: loss=84038.79775295618, w0=73.30400000000007, w1=14.701995754347605\n",
      "SubGD iter. 347/499: loss=84038.78584343955, w0=73.30750000000008, w1=14.702459021852679\n",
      "SubGD iter. 348/499: loss=84038.79113906407, w0=73.30400000000007, w1=14.706367451673682\n",
      "SubGD iter. 349/499: loss=84039.0699985571, w0=73.30400000000007, w1=14.704064828179025\n",
      "SubGD iter. 350/499: loss=84038.76702284523, w0=73.30400000000007, w1=14.701762204684368\n",
      "SubGD iter. 351/499: loss=84038.79202606638, w0=73.30750000000008, w1=14.702225472189442\n",
      "SubGD iter. 352/499: loss=84038.84329977656, w0=73.30400000000007, w1=14.706133902010444\n",
      "SubGD iter. 353/499: loss=84039.03926844617, w0=73.30400000000007, w1=14.703831278515787\n",
      "SubGD iter. 354/499: loss=84038.7372526566, w0=73.30750000000008, w1=14.704294546020861\n",
      "SubGD iter. 355/499: loss=84038.79724877092, w0=73.30750000000008, w1=14.701991922526204\n",
      "SubGD iter. 356/499: loss=84038.89546048903, w0=73.30400000000007, w1=14.705900352347207\n",
      "SubGD iter. 357/499: loss=84039.0085383352, w0=73.30400000000007, w1=14.70359772885255\n",
      "SubGD iter. 358/499: loss=84038.74343528342, w0=73.30750000000008, w1=14.704060996357624\n",
      "SubGD iter. 359/499: loss=84038.76651865996, w0=73.30750000000008, w1=14.701758372862967\n",
      "SubGD iter. 360/499: loss=84038.94762120151, w0=73.30400000000007, w1=14.70566680268397\n",
      "SubGD iter. 361/499: loss=84038.97780822426, w0=73.30400000000007, w1=14.703364179189313\n",
      "SubGD iter. 362/499: loss=84038.74961791026, w0=73.30750000000008, w1=14.703827446694387\n",
      "SubGD iter. 363/499: loss=84038.73578854902, w0=73.30750000000008, w1=14.70152482319973\n",
      "SubGD iter. 364/499: loss=84038.999781914, w0=73.30400000000007, w1=14.705433253020733\n",
      "SubGD iter. 365/499: loss=84038.94707811328, w0=73.30400000000007, w1=14.703130629526076\n",
      "SubGD iter. 366/499: loss=84038.75580053711, w0=73.30750000000008, w1=14.70359389703115\n",
      "SubGD iter. 367/499: loss=84038.70505843803, w0=73.30750000000008, w1=14.701291273536492\n",
      "SubGD iter. 368/499: loss=84039.05194262648, w0=73.30400000000007, w1=14.705199703357495\n",
      "SubGD iter. 369/499: loss=84038.91634800233, w0=73.30400000000007, w1=14.702897079862838\n",
      "SubGD iter. 370/499: loss=84038.76198316396, w0=73.30750000000008, w1=14.703360347367912\n",
      "SubGD iter. 371/499: loss=84038.6743283271, w0=73.30750000000008, w1=14.701057723873255\n",
      "SubGD iter. 372/499: loss=84039.10410333896, w0=73.30400000000007, w1=14.704966153694258\n",
      "SubGD iter. 373/499: loss=84038.88561789137, w0=73.30400000000007, w1=14.702663530199601\n",
      "SubGD iter. 374/499: loss=84038.76816579081, w0=73.30750000000008, w1=14.703126797704675\n",
      "SubGD iter. 375/499: loss=84038.64359821613, w0=73.30750000000008, w1=14.700824174210018\n",
      "SubGD iter. 376/499: loss=84039.15626405146, w0=73.30400000000007, w1=14.70473260403102\n",
      "SubGD iter. 377/499: loss=84038.85488778043, w0=73.30400000000007, w1=14.702429980536364\n",
      "SubGD iter. 378/499: loss=84038.77434841765, w0=73.30750000000008, w1=14.702893248041438\n",
      "SubGD iter. 379/499: loss=84038.69415948776, w0=73.30400000000007, w1=14.70680167786244\n",
      "SubGD iter. 380/499: loss=84039.12713338135, w0=73.30400000000007, w1=14.704499054367783\n",
      "SubGD iter. 381/499: loss=84038.82415766946, w0=73.30400000000007, w1=14.702196430873126\n",
      "SubGD iter. 382/499: loss=84038.7805310445, w0=73.30750000000008, w1=14.7026596983782\n",
      "SubGD iter. 383/499: loss=84038.74632020024, w0=73.30400000000007, w1=14.706568128199203\n",
      "SubGD iter. 384/499: loss=84039.0964032704, w0=73.30400000000007, w1=14.704265504704546\n",
      "SubGD iter. 385/499: loss=84038.7934275585, w0=73.30400000000007, w1=14.701962881209889\n",
      "SubGD iter. 386/499: loss=84038.78671367133, w0=73.30750000000008, w1=14.702426148714963\n",
      "SubGD iter. 387/499: loss=84038.79848091274, w0=73.30400000000007, w1=14.706334578535966\n",
      "SubGD iter. 388/499: loss=84039.06567315945, w0=73.30400000000007, w1=14.704031955041309\n",
      "SubGD iter. 389/499: loss=84038.76269744756, w0=73.30400000000007, w1=14.701729331546652\n",
      "SubGD iter. 390/499: loss=84038.79289629818, w0=73.30750000000008, w1=14.702192599051726\n",
      "SubGD iter. 391/499: loss=84038.8506416252, w0=73.30400000000007, w1=14.706101028872729\n",
      "SubGD iter. 392/499: loss=84039.0349430485, w0=73.30400000000007, w1=14.703798405378071\n",
      "SubGD iter. 393/499: loss=84038.73812288836, w0=73.30750000000008, w1=14.704261672883145\n",
      "SubGD iter. 394/499: loss=84038.79292337324, w0=73.30750000000008, w1=14.701959049388488\n",
      "SubGD iter. 395/499: loss=84038.9028023377, w0=73.30400000000007, w1=14.705867479209491\n",
      "SubGD iter. 396/499: loss=84039.00421293752, w0=73.30400000000007, w1=14.703564855714834\n",
      "SubGD iter. 397/499: loss=84038.74430551523, w0=73.30750000000008, w1=14.704028123219908\n",
      "SubGD iter. 398/499: loss=84038.7621932623, w0=73.30750000000008, w1=14.701725499725251\n",
      "SubGD iter. 399/499: loss=84038.95496305017, w0=73.30400000000007, w1=14.705633929546254\n",
      "SubGD iter. 400/499: loss=84038.97348282656, w0=73.30400000000007, w1=14.703331306051597\n",
      "SubGD iter. 401/499: loss=84038.75048814208, w0=73.30750000000008, w1=14.70379457355667\n",
      "SubGD iter. 402/499: loss=84038.73146315134, w0=73.30750000000008, w1=14.701491950062014\n",
      "SubGD iter. 403/499: loss=84039.00712376268, w0=73.30400000000007, w1=14.705400379883017\n",
      "SubGD iter. 404/499: loss=84038.94275271562, w0=73.30400000000007, w1=14.70309775638836\n",
      "SubGD iter. 405/499: loss=84038.75667076891, w0=73.30750000000008, w1=14.703561023893434\n",
      "SubGD iter. 406/499: loss=84038.70073304037, w0=73.30750000000008, w1=14.701258400398777\n",
      "SubGD iter. 407/499: loss=84039.05928447515, w0=73.30400000000007, w1=14.70516683021978\n",
      "SubGD iter. 408/499: loss=84038.91202260465, w0=73.30400000000007, w1=14.702864206725122\n",
      "SubGD iter. 409/499: loss=84038.76285339576, w0=73.30750000000008, w1=14.703327474230196\n",
      "SubGD iter. 410/499: loss=84038.67000292943, w0=73.30750000000008, w1=14.70102485073554\n",
      "SubGD iter. 411/499: loss=84039.11144518763, w0=73.30400000000007, w1=14.704933280556542\n",
      "SubGD iter. 412/499: loss=84038.88129249371, w0=73.30400000000007, w1=14.702630657061885\n",
      "SubGD iter. 413/499: loss=84038.76903602258, w0=73.30750000000008, w1=14.703093924566959\n",
      "SubGD iter. 414/499: loss=84038.64934062396, w0=73.30400000000007, w1=14.707002354387962\n",
      "SubGD iter. 415/499: loss=84039.15353809463, w0=73.30400000000007, w1=14.704699730893305\n",
      "SubGD iter. 416/499: loss=84038.85056238274, w0=73.30400000000007, w1=14.702397107398648\n",
      "SubGD iter. 417/499: loss=84038.77521864945, w0=73.30750000000008, w1=14.702860374903722\n",
      "SubGD iter. 418/499: loss=84038.70150133644, w0=73.30400000000007, w1=14.706768804724724\n",
      "SubGD iter. 419/499: loss=84039.12280798367, w0=73.30400000000007, w1=14.704466181230067\n",
      "SubGD iter. 420/499: loss=84038.8198322718, w0=73.30400000000007, w1=14.70216355773541\n",
      "SubGD iter. 421/499: loss=84038.78140127628, w0=73.30750000000008, w1=14.702626825240484\n",
      "SubGD iter. 422/499: loss=84038.75366204891, w0=73.30400000000007, w1=14.706535255061487\n",
      "SubGD iter. 423/499: loss=84039.09207787273, w0=73.30400000000007, w1=14.70423263156683\n",
      "SubGD iter. 424/499: loss=84038.78910216084, w0=73.30400000000007, w1=14.701930008072173\n",
      "SubGD iter. 425/499: loss=84038.78758390313, w0=73.30750000000008, w1=14.702393275577247\n",
      "SubGD iter. 426/499: loss=84038.8058227614, w0=73.30400000000007, w1=14.70630170539825\n",
      "SubGD iter. 427/499: loss=84039.06134776177, w0=73.30400000000007, w1=14.703999081903593\n",
      "SubGD iter. 428/499: loss=84038.75837204988, w0=73.30400000000007, w1=14.701696458408936\n",
      "SubGD iter. 429/499: loss=84038.79376652998, w0=73.30750000000008, w1=14.70215972591401\n",
      "SubGD iter. 430/499: loss=84038.85798347388, w0=73.30400000000007, w1=14.706068155735013\n",
      "SubGD iter. 431/499: loss=84039.03061765082, w0=73.30400000000007, w1=14.703765532240356\n",
      "SubGD iter. 432/499: loss=84038.73899312018, w0=73.30750000000008, w1=14.70422879974543\n",
      "SubGD iter. 433/499: loss=84038.78859797558, w0=73.30750000000008, w1=14.701926176250772\n",
      "SubGD iter. 434/499: loss=84038.91014418636, w0=73.30400000000007, w1=14.705834606071775\n",
      "SubGD iter. 435/499: loss=84038.99988753986, w0=73.30400000000007, w1=14.703531982577118\n",
      "SubGD iter. 436/499: loss=84038.74517574701, w0=73.30750000000008, w1=14.703995250082192\n",
      "SubGD iter. 437/499: loss=84038.75786786462, w0=73.30750000000008, w1=14.701692626587535\n",
      "SubGD iter. 438/499: loss=84038.96230489886, w0=73.30400000000007, w1=14.705601056408538\n",
      "SubGD iter. 439/499: loss=84038.96915742889, w0=73.30400000000007, w1=14.703298432913881\n",
      "SubGD iter. 440/499: loss=84038.75135837388, w0=73.30750000000008, w1=14.703761700418955\n",
      "SubGD iter. 441/499: loss=84038.72713775367, w0=73.30750000000008, w1=14.701459076924298\n",
      "SubGD iter. 442/499: loss=84039.01446561136, w0=73.30400000000007, w1=14.7053675067453\n",
      "SubGD iter. 443/499: loss=84038.93842731795, w0=73.30400000000007, w1=14.703064883250644\n",
      "SubGD iter. 444/499: loss=84038.7575410007, w0=73.30750000000008, w1=14.703528150755718\n",
      "SubGD iter. 445/499: loss=84038.69640764271, w0=73.30750000000008, w1=14.70122552726106\n",
      "SubGD iter. 446/499: loss=84039.06662632382, w0=73.30400000000007, w1=14.705133957082063\n",
      "SubGD iter. 447/499: loss=84038.90769720699, w0=73.30400000000007, w1=14.702831333587406\n",
      "SubGD iter. 448/499: loss=84038.76372362753, w0=73.30750000000008, w1=14.70329460109248\n",
      "SubGD iter. 449/499: loss=84038.66567753175, w0=73.30750000000008, w1=14.700991977597823\n",
      "SubGD iter. 450/499: loss=84039.11878703632, w0=73.30400000000007, w1=14.704900407418826\n",
      "SubGD iter. 451/499: loss=84038.87696709603, w0=73.30400000000007, w1=14.702597783924169\n",
      "SubGD iter. 452/499: loss=84038.7699062544, w0=73.30750000000008, w1=14.703061051429243\n",
      "SubGD iter. 453/499: loss=84038.65668247263, w0=73.30400000000007, w1=14.706969481250246\n",
      "SubGD iter. 454/499: loss=84039.14921269697, w0=73.30400000000007, w1=14.704666857755589\n",
      "SubGD iter. 455/499: loss=84038.84623698507, w0=73.30400000000007, w1=14.702364234260932\n",
      "SubGD iter. 456/499: loss=84038.77608888125, w0=73.30750000000008, w1=14.702827501766006\n",
      "SubGD iter. 457/499: loss=84038.70884318513, w0=73.30400000000007, w1=14.706735931587009\n",
      "SubGD iter. 458/499: loss=84039.11848258603, w0=73.30400000000007, w1=14.704433308092351\n",
      "SubGD iter. 459/499: loss=84038.81550687412, w0=73.30400000000007, w1=14.702130684597694\n",
      "SubGD iter. 460/499: loss=84038.78227150807, w0=73.30750000000008, w1=14.702593952102768\n",
      "SubGD iter. 461/499: loss=84038.7610038976, w0=73.30400000000007, w1=14.706502381923771\n",
      "SubGD iter. 462/499: loss=84039.08775247505, w0=73.30400000000007, w1=14.704199758429114\n",
      "SubGD iter. 463/499: loss=84038.78477676318, w0=73.30400000000007, w1=14.701897134934457\n",
      "SubGD iter. 464/499: loss=84038.78845413495, w0=73.30750000000008, w1=14.702360402439531\n",
      "SubGD iter. 465/499: loss=84038.81316461007, w0=73.30400000000007, w1=14.706268832260534\n",
      "SubGD iter. 466/499: loss=84039.05702236411, w0=73.30400000000007, w1=14.703966208765877\n",
      "SubGD iter. 467/499: loss=84038.75404665219, w0=73.30400000000007, w1=14.70166358527122\n",
      "SubGD iter. 468/499: loss=84038.79463676178, w0=73.30750000000008, w1=14.702126852776294\n",
      "SubGD iter. 469/499: loss=84038.86532532256, w0=73.30400000000007, w1=14.706035282597297\n",
      "SubGD iter. 470/499: loss=84039.02629225313, w0=73.30400000000007, w1=14.70373265910264\n",
      "SubGD iter. 471/499: loss=84038.73986335196, w0=73.30750000000008, w1=14.704195926607714\n",
      "SubGD iter. 472/499: loss=84038.78427257789, w0=73.30750000000008, w1=14.701893303113057\n",
      "SubGD iter. 473/499: loss=84038.91748603505, w0=73.30400000000007, w1=14.70580173293406\n",
      "SubGD iter. 474/499: loss=84038.99556214218, w0=73.30400000000007, w1=14.703499109439402\n",
      "SubGD iter. 475/499: loss=84038.74604597881, w0=73.30750000000008, w1=14.703962376944476\n",
      "SubGD iter. 476/499: loss=84038.75354246695, w0=73.30750000000008, w1=14.70165975344982\n",
      "SubGD iter. 477/499: loss=84038.96964674754, w0=73.30400000000007, w1=14.705568183270822\n",
      "SubGD iter. 478/499: loss=84038.96483203123, w0=73.30400000000007, w1=14.703265559776165\n",
      "SubGD iter. 479/499: loss=84038.75222860566, w0=73.30750000000008, w1=14.703728827281239\n",
      "SubGD iter. 480/499: loss=84038.722812356, w0=73.30750000000008, w1=14.701426203786582\n",
      "SubGD iter. 481/499: loss=84039.02180746001, w0=73.30400000000007, w1=14.705334633607585\n",
      "SubGD iter. 482/499: loss=84038.93410192027, w0=73.30400000000007, w1=14.703032010112928\n",
      "SubGD iter. 483/499: loss=84038.75841123251, w0=73.30750000000008, w1=14.703495277618002\n",
      "SubGD iter. 484/499: loss=84038.69208224503, w0=73.30750000000008, w1=14.701192654123345\n",
      "SubGD iter. 485/499: loss=84039.07396817248, w0=73.30400000000007, w1=14.705101083944347\n",
      "SubGD iter. 486/499: loss=84038.90337180931, w0=73.30400000000007, w1=14.70279846044969\n",
      "SubGD iter. 487/499: loss=84038.76459385935, w0=73.30750000000008, w1=14.703261727954764\n",
      "SubGD iter. 488/499: loss=84038.66135213409, w0=73.30750000000008, w1=14.700959104460107\n",
      "SubGD iter. 489/499: loss=84039.126128885, w0=73.30400000000007, w1=14.70486753428111\n",
      "SubGD iter. 490/499: loss=84038.87264169837, w0=73.30400000000007, w1=14.702564910786453\n",
      "SubGD iter. 491/499: loss=84038.7707764862, w0=73.30750000000008, w1=14.703028178291527\n",
      "SubGD iter. 492/499: loss=84038.66402432129, w0=73.30400000000007, w1=14.70693660811253\n",
      "SubGD iter. 493/499: loss=84039.1448872993, w0=73.30400000000007, w1=14.704633984617873\n",
      "SubGD iter. 494/499: loss=84038.8419115874, w0=73.30400000000007, w1=14.702331361123216\n",
      "SubGD iter. 495/499: loss=84038.77695911303, w0=73.30750000000008, w1=14.70279462862829\n",
      "SubGD iter. 496/499: loss=84038.71618503379, w0=73.30400000000007, w1=14.706703058449293\n",
      "SubGD iter. 497/499: loss=84039.11415718835, w0=73.30400000000007, w1=14.704400434954636\n",
      "SubGD iter. 498/499: loss=84038.81118147644, w0=73.30400000000007, w1=14.702097811459979\n",
      "SubGD iter. 499/499: loss=84038.78314173988, w0=73.30750000000008, w1=14.702561078965052\n",
      "SubGD: execution time=0.063 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ee931ac02e4c1b81166b353a5b4b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses,\n",
    "        subgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic SubGradient Descent algorithm (SubSGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1) :\n",
    "        \n",
    "            grad = compute_subgradient_mae(y_batch, tx_batch, w)\n",
    "            w = w - gamma*grad\n",
    "            loss = compute_loss_mae(y, tx, w)\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "        # TODO: implement stochastic subgradient descent.\n",
    "        # ***************************************************\n",
    "        \n",
    "        print(\n",
    "            \"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=1465645.449194623, w0=0.35, w1=0.226825536331064\n",
      "SubSGD iter. 1/499: loss=1458645.449194623, w0=0.7, w1=-0.13653973849254258\n",
      "SubSGD iter. 2/499: loss=1451645.449194623, w0=1.0499999999999998, w1=0.19690632131180513\n",
      "SubSGD iter. 3/499: loss=1444645.4491946232, w0=1.4, w1=-0.18423266425297807\n",
      "SubSGD iter. 4/499: loss=1437645.449194623, w0=1.75, w1=0.22076445930001865\n",
      "SubSGD iter. 5/499: loss=1430645.449194623, w0=2.1, w1=0.330741015640327\n",
      "SubSGD iter. 6/499: loss=1423645.449194623, w0=2.45, w1=0.20243787907847488\n",
      "SubSGD iter. 7/499: loss=1416645.4491946227, w0=2.8000000000000003, w1=0.8052959194590334\n",
      "SubSGD iter. 8/499: loss=1409645.4491946227, w0=3.1500000000000004, w1=0.7271991745544539\n",
      "SubSGD iter. 9/499: loss=1402645.449194623, w0=3.5000000000000004, w1=0.14093900993230968\n",
      "SubSGD iter. 10/499: loss=1395645.4491946232, w0=3.8500000000000005, w1=-0.262207974053628\n",
      "SubSGD iter. 11/499: loss=1388645.449194623, w0=4.2, w1=-0.5506686164457271\n",
      "SubSGD iter. 12/499: loss=1381645.4491946232, w0=4.55, w1=-0.7228613064977427\n",
      "SubSGD iter. 13/499: loss=1374645.4491946232, w0=4.8999999999999995, w1=-1.1158990148971935\n",
      "SubSGD iter. 14/499: loss=1367645.4491946232, w0=5.249999999999999, w1=-1.0534768794390201\n",
      "SubSGD iter. 15/499: loss=1360645.4491946232, w0=5.599999999999999, w1=-1.38643522295309\n",
      "SubSGD iter. 16/499: loss=1353645.4491946232, w0=5.949999999999998, w1=-1.3240130874949168\n",
      "SubSGD iter. 17/499: loss=1346645.4491946232, w0=6.299999999999998, w1=-1.4427357537505372\n",
      "SubSGD iter. 18/499: loss=1339645.4491946232, w0=6.649999999999998, w1=-1.7396962161885254\n",
      "SubSGD iter. 19/499: loss=1332645.4491946232, w0=6.999999999999997, w1=-1.7421044301852229\n",
      "SubSGD iter. 20/499: loss=1325645.4491946232, w0=7.349999999999997, w1=-1.353399631747822\n",
      "SubSGD iter. 21/499: loss=1318645.4491946232, w0=7.699999999999997, w1=-1.7565466157337597\n",
      "SubSGD iter. 22/499: loss=1311645.4491946232, w0=8.049999999999997, w1=-1.2123738698922961\n",
      "SubSGD iter. 23/499: loss=1304645.4491946232, w0=8.399999999999997, w1=-1.2958571808318944\n",
      "SubSGD iter. 24/499: loss=1297645.4491946232, w0=8.749999999999996, w1=-1.6571357606285764\n",
      "SubSGD iter. 25/499: loss=1290645.4491946232, w0=9.099999999999996, w1=-1.3675177580853137\n",
      "SubSGD iter. 26/499: loss=1283645.4491946232, w0=9.449999999999996, w1=-0.9099364516380186\n",
      "SubSGD iter. 27/499: loss=1276645.4491946232, w0=9.799999999999995, w1=-0.07329918099899912\n",
      "SubSGD iter. 28/499: loss=1269645.4491946232, w0=10.149999999999995, w1=-0.4366644558226057\n",
      "SubSGD iter. 29/499: loss=1262645.4491946232, w0=10.499999999999995, w1=-0.7336249182605941\n",
      "SubSGD iter. 30/499: loss=1255645.449194623, w0=10.849999999999994, w1=-0.08840212763842137\n",
      "SubSGD iter. 31/499: loss=1248645.4491946232, w0=11.199999999999994, w1=-0.4486217534495582\n",
      "SubSGD iter. 32/499: loss=1241645.4491946232, w0=11.549999999999994, w1=-0.9389395549719288\n",
      "SubSGD iter. 33/499: loss=1234645.4491946232, w0=11.899999999999993, w1=-1.4292573564942994\n",
      "SubSGD iter. 34/499: loss=1227645.4491946232, w0=12.249999999999993, w1=-1.6376173800331777\n",
      "SubSGD iter. 35/499: loss=1220645.4491946232, w0=12.599999999999993, w1=-1.5812965132993018\n",
      "SubSGD iter. 36/499: loss=1213645.4491946232, w0=12.949999999999992, w1=-1.1913106332939905\n",
      "SubSGD iter. 37/499: loss=1206645.4491946232, w0=13.299999999999992, w1=-1.594457617279928\n",
      "SubSGD iter. 38/499: loss=1199645.4491946232, w0=13.649999999999991, w1=-1.0250767685028044\n",
      "SubSGD iter. 39/499: loss=1192645.4491946232, w0=13.999999999999991, w1=-0.7023187760887213\n",
      "SubSGD iter. 40/499: loss=1185645.4491946232, w0=14.34999999999999, w1=-1.1262685824630965\n",
      "SubSGD iter. 41/499: loss=1178645.4491946232, w0=14.69999999999999, w1=-1.3568601384361088\n",
      "SubSGD iter. 42/499: loss=1171645.4491946232, w0=15.04999999999999, w1=-1.6915090886250583\n",
      "SubSGD iter. 43/499: loss=1164645.4491946234, w0=15.39999999999999, w1=-1.8198122251869104\n",
      "SubSGD iter. 44/499: loss=1157645.4491946232, w0=15.74999999999999, w1=-1.4725593887379056\n",
      "SubSGD iter. 45/499: loss=1150645.4491946234, w0=16.09999999999999, w1=-1.7928807895438605\n",
      "SubSGD iter. 46/499: loss=1143645.4491946232, w0=16.449999999999992, w1=-1.248708043702397\n",
      "SubSGD iter. 47/499: loss=1136645.4491946232, w0=16.799999999999994, w1=-1.834968208324541\n",
      "SubSGD iter. 48/499: loss=1129645.4491946232, w0=17.149999999999995, w1=-2.2923176633299205\n",
      "SubSGD iter. 49/499: loss=1122645.4491946232, w0=17.499999999999996, w1=-2.3758009742695188\n",
      "SubSGD iter. 50/499: loss=1115645.4491946232, w0=17.849999999999998, w1=-2.183001250374652\n",
      "SubSGD iter. 51/499: loss=1108645.4491946232, w0=18.2, w1=-2.09109592321091\n",
      "SubSGD iter. 52/499: loss=1101645.449194623, w0=18.55, w1=-1.8711478449947574\n",
      "SubSGD iter. 53/499: loss=1094645.449194623, w0=18.900000000000002, w1=-1.3632965187561483\n",
      "SubSGD iter. 54/499: loss=1087645.449194623, w0=19.250000000000004, w1=-2.004432979892472\n",
      "SubSGD iter. 55/499: loss=1080645.449194623, w0=19.600000000000005, w1=-1.5551839193028183\n",
      "SubSGD iter. 56/499: loss=1073645.449194623, w0=19.950000000000006, w1=-1.3793308107624263\n",
      "SubSGD iter. 57/499: loss=1066645.449194623, w0=20.300000000000008, w1=-0.9090280648085909\n",
      "SubSGD iter. 58/499: loss=1059645.4491946227, w0=20.65000000000001, w1=-1.2168613231409449\n",
      "SubSGD iter. 59/499: loss=1052645.4491946227, w0=21.00000000000001, w1=-0.7719649886075337\n",
      "SubSGD iter. 60/499: loss=1045645.4491946228, w0=21.350000000000012, w1=-0.26411366236892453\n",
      "SubSGD iter. 61/499: loss=1038645.4491946228, w0=21.700000000000014, w1=0.19346764407837064\n",
      "SubSGD iter. 62/499: loss=1031645.4491946228, w0=22.050000000000015, w1=0.04117569852093245\n",
      "SubSGD iter. 63/499: loss=1024645.4491946226, w0=22.400000000000016, w1=-0.24728494387116667\n",
      "SubSGD iter. 64/499: loss=1017645.4491946226, w0=22.750000000000018, w1=-0.12557961679580726\n",
      "SubSGD iter. 65/499: loss=1010645.4491946226, w0=23.10000000000002, w1=0.1396378477425373\n",
      "SubSGD iter. 66/499: loss=1003645.4491946226, w0=23.45000000000002, w1=0.23144590036350826\n",
      "SubSGD iter. 67/499: loss=996645.4491946225, w0=23.800000000000022, w1=0.636443023916505\n",
      "SubSGD iter. 68/499: loss=989645.4491946225, w0=24.150000000000023, w1=0.9610825772316556\n",
      "SubSGD iter. 69/499: loss=982645.4491946225, w0=24.500000000000025, w1=1.234611680362513\n",
      "SubSGD iter. 70/499: loss=975645.4491946225, w0=24.850000000000026, w1=0.6483515157403686\n",
      "SubSGD iter. 71/499: loss=968645.4491946225, w0=25.200000000000028, w1=0.9877038431833559\n",
      "SubSGD iter. 72/499: loss=961645.4491946225, w0=25.55000000000003, w1=0.6613064847217882\n",
      "SubSGD iter. 73/499: loss=954645.4491946222, w0=25.90000000000003, w1=0.9378955846947872\n",
      "SubSGD iter. 74/499: loss=947645.4491946222, w0=26.250000000000032, w1=1.2640575068698825\n",
      "SubSGD iter. 75/499: loss=940645.4491946222, w0=26.600000000000033, w1=1.1805741959302842\n",
      "SubSGD iter. 76/499: loss=933645.449194622, w0=26.950000000000035, w1=1.8320481328832523\n",
      "SubSGD iter. 77/499: loss=926645.4491946222, w0=27.300000000000036, w1=1.3716714033332877\n",
      "SubSGD iter. 78/499: loss=919645.4491946222, w0=27.650000000000038, w1=1.6293727254401615\n",
      "SubSGD iter. 79/499: loss=912645.449194622, w0=28.00000000000004, w1=1.7981645498633998\n",
      "SubSGD iter. 80/499: loss=905645.449194622, w0=28.35000000000004, w1=1.8544854165972757\n",
      "SubSGD iter. 81/499: loss=898645.449194622, w0=28.700000000000042, w1=2.128014519728133\n",
      "SubSGD iter. 82/499: loss=891645.449194622, w0=29.050000000000043, w1=1.8831268637242955\n",
      "SubSGD iter. 83/499: loss=884645.449194622, w0=29.400000000000045, w1=2.227538441356153\n",
      "SubSGD iter. 84/499: loss=877645.4491946219, w0=29.750000000000046, w1=2.3550257801301346\n",
      "SubSGD iter. 85/499: loss=870645.449194622, w0=30.100000000000048, w1=1.7687656155079905\n",
      "SubSGD iter. 86/499: loss=863645.4491946219, w0=30.45000000000005, w1=2.1193230032271786\n",
      "SubSGD iter. 87/499: loss=856645.4491946216, w0=30.80000000000005, w1=2.5093088832324897\n",
      "SubSGD iter. 88/499: loss=849645.4491946216, w0=31.150000000000052, w1=2.048932153682525\n",
      "SubSGD iter. 89/499: loss=842645.4491946216, w0=31.500000000000053, w1=2.0525739888449026\n",
      "SubSGD iter. 90/499: loss=835645.4491946216, w0=31.850000000000055, w1=2.476313696162029\n",
      "SubSGD iter. 91/499: loss=828645.4491946216, w0=32.20000000000005, w1=2.9610299143857493\n",
      "SubSGD iter. 92/499: loss=821645.4491946216, w0=32.550000000000054, w1=3.250647916929012\n",
      "SubSGD iter. 93/499: loss=814645.4491946216, w0=32.900000000000055, w1=2.989899814962208\n",
      "SubSGD iter. 94/499: loss=807645.4491946216, w0=33.25000000000006, w1=3.22240991531954\n",
      "SubSGD iter. 95/499: loss=800645.4491946216, w0=33.60000000000006, w1=3.5939143048944113\n",
      "SubSGD iter. 96/499: loss=793645.4491946216, w0=33.95000000000006, w1=3.7678280120088763\n",
      "SubSGD iter. 97/499: loss=786645.4491946216, w0=34.30000000000006, w1=4.275679338247485\n",
      "SubSGD iter. 98/499: loss=779645.4491946215, w0=34.65000000000006, w1=4.02011776480286\n",
      "SubSGD iter. 99/499: loss=772645.4491946214, w0=35.000000000000064, w1=4.4165945043751185\n",
      "SubSGD iter. 100/499: loss=765645.4491946215, w0=35.350000000000065, w1=3.824468688279327\n",
      "SubSGD iter. 101/499: loss=758645.4491946213, w0=35.70000000000007, w1=4.157914748083675\n",
      "SubSGD iter. 102/499: loss=751645.4491946215, w0=36.05000000000007, w1=4.214235614817551\n",
      "SubSGD iter. 103/499: loss=744645.4491946213, w0=36.40000000000007, w1=4.276657750275724\n",
      "SubSGD iter. 104/499: loss=737645.4491946214, w0=36.75000000000007, w1=4.386634306616033\n",
      "SubSGD iter. 105/499: loss=730645.4491946212, w0=37.10000000000007, w1=4.970404958241791\n",
      "SubSGD iter. 106/499: loss=723645.4491946212, w0=37.450000000000074, w1=5.514710739171717\n",
      "SubSGD iter. 107/499: loss=716645.4491946212, w0=37.800000000000075, w1=5.427279319152544\n",
      "SubSGD iter. 108/499: loss=709645.4491946212, w0=38.15000000000008, w1=5.897624000297478\n",
      "SubSGD iter. 109/499: loss=702645.4491946212, w0=38.50000000000008, w1=6.382340218521199\n",
      "SubSGD iter. 110/499: loss=695645.4491946211, w0=38.85000000000008, w1=6.9709605481920125\n",
      "SubSGD iter. 111/499: loss=688645.4491946212, w0=39.20000000000008, w1=6.758200085048645\n",
      "SubSGD iter. 112/499: loss=681645.449194621, w0=39.55000000000008, w1=6.199729836449159\n",
      "SubSGD iter. 113/499: loss=674645.4491946211, w0=39.900000000000084, w1=5.7423803814437795\n",
      "SubSGD iter. 114/499: loss=667645.449194621, w0=40.250000000000085, w1=6.060931562960219\n",
      "SubSGD iter. 115/499: loss=660645.449194621, w0=40.60000000000009, w1=5.577502529051979\n",
      "SubSGD iter. 116/499: loss=653645.449194621, w0=40.95000000000009, w1=5.125771849368807\n",
      "SubSGD iter. 117/499: loss=646645.449194621, w0=41.30000000000009, w1=4.889552629705269\n",
      "SubSGD iter. 118/499: loss=639645.449194621, w0=41.65000000000009, w1=4.644664973701432\n",
      "SubSGD iter. 119/499: loss=632645.449194621, w0=42.00000000000009, w1=4.287961294701484\n",
      "SubSGD iter. 120/499: loss=625645.449194621, w0=42.350000000000094, w1=4.355684339422864\n",
      "SubSGD iter. 121/499: loss=618645.4491946208, w0=42.700000000000095, w1=4.119465119759327\n",
      "SubSGD iter. 122/499: loss=611645.4491946208, w0=43.0500000000001, w1=3.726427411359876\n",
      "SubSGD iter. 123/499: loss=604645.4491946208, w0=43.4000000000001, w1=3.2660506818099115\n",
      "SubSGD iter. 124/499: loss=597645.4491946208, w0=43.7500000000001, w1=3.0298314621463738\n",
      "SubSGD iter. 125/499: loss=590645.4491946208, w0=44.1000000000001, w1=3.4185362605837746\n",
      "SubSGD iter. 126/499: loss=583645.4491946208, w0=44.4500000000001, w1=3.88883900653761\n",
      "SubSGD iter. 127/499: loss=576645.4491946208, w0=44.800000000000104, w1=4.1645161872016\n",
      "SubSGD iter. 128/499: loss=569645.4491946207, w0=45.150000000000105, w1=3.771478478802149\n",
      "SubSGD iter. 129/499: loss=562645.4491946206, w0=45.50000000000011, w1=3.8175429329103285\n",
      "SubSGD iter. 130/499: loss=555645.4491946206, w0=45.85000000000011, w1=4.050053033267661\n",
      "SubSGD iter. 131/499: loss=548645.4491946206, w0=46.20000000000011, w1=4.182765376126812\n",
      "SubSGD iter. 132/499: loss=541645.4491946206, w0=46.55000000000011, w1=3.894304733734713\n",
      "SubSGD iter. 133/499: loss=534645.4491946206, w0=46.90000000000011, w1=4.031445150881895\n",
      "SubSGD iter. 134/499: loss=527645.4491946205, w0=47.250000000000114, w1=4.273047169342659\n",
      "SubSGD iter. 135/499: loss=520645.4491946204, w0=47.600000000000115, w1=4.0478314289033\n",
      "SubSGD iter. 136/499: loss=513645.4491946204, w0=47.95000000000012, w1=4.122711935927592\n",
      "SubSGD iter. 137/499: loss=506645.4491946204, w0=48.30000000000012, w1=3.8864927162640543\n",
      "SubSGD iter. 138/499: loss=499645.4491946204, w0=48.65000000000012, w1=3.346475826764026\n",
      "SubSGD iter. 139/499: loss=492645.4491946204, w0=49.00000000000012, w1=2.989772147764078\n",
      "SubSGD iter. 140/499: loss=485645.4491946203, w0=49.35000000000012, w1=3.5038443422606544\n",
      "SubSGD iter. 141/499: loss=478645.4491946204, w0=49.700000000000124, w1=2.8153200993843983\n",
      "SubSGD iter. 142/499: loss=471645.4491946203, w0=50.050000000000125, w1=2.8295060889578116\n",
      "SubSGD iter. 143/499: loss=464645.4491946203, w0=50.40000000000013, w1=2.515638248166376\n",
      "SubSGD iter. 144/499: loss=457682.87141387974, w0=50.75000000000013, w1=2.300876873905635\n",
      "SubSGD iter. 145/499: loss=450682.236595202, w0=51.10000000000013, w1=2.4936765978005018\n",
      "SubSGD iter. 146/499: loss=444043.57976845733, w0=51.45000000000013, w1=2.184555073515077\n",
      "SubSGD iter. 147/499: loss=437268.2642179205, w0=51.80000000000013, w1=2.1881969086774546\n",
      "SubSGD iter. 148/499: loss=430752.8148187179, w0=52.150000000000134, w1=2.020687434953849\n",
      "SubSGD iter. 149/499: loss=423820.9552916629, w0=52.500000000000135, w1=2.194182295110485\n",
      "SubSGD iter. 150/499: loss=417935.5615822797, w0=52.850000000000136, w1=1.7338055655605205\n",
      "SubSGD iter. 151/499: loss=411267.378282488, w0=53.20000000000014, w1=1.8015286102819006\n",
      "SubSGD iter. 152/499: loss=405563.4535416775, w0=53.55000000000014, w1=1.5726876352678236\n",
      "SubSGD iter. 153/499: loss=398313.1501663834, w0=53.90000000000014, w1=1.8623056378110863\n",
      "SubSGD iter. 154/499: loss=390903.09571113996, w0=54.25000000000014, w1=2.2067172154429437\n",
      "SubSGD iter. 155/499: loss=383324.13721546956, w0=54.60000000000014, w1=2.6304569227600703\n",
      "SubSGD iter. 156/499: loss=377010.6129545631, w0=54.950000000000145, w1=2.6058056501164466\n",
      "SubSGD iter. 157/499: loss=371470.29292557837, w0=55.300000000000146, w1=2.3835375628722324\n",
      "SubSGD iter. 158/499: loss=365980.5405791206, w0=55.65000000000015, w1=2.2552344263103805\n",
      "SubSGD iter. 159/499: loss=362950.69543633744, w0=56.00000000000015, w1=1.663108610214589\n",
      "SubSGD iter. 160/499: loss=355544.35530547786, w0=56.35000000000015, w1=1.9816597917310281\n",
      "SubSGD iter. 161/499: loss=349225.51057108346, w0=56.70000000000015, w1=2.1033651188063875\n",
      "SubSGD iter. 162/499: loss=346029.78065188084, w0=57.05000000000015, w1=1.7431454929952508\n",
      "SubSGD iter. 163/499: loss=336418.0649218144, w0=57.400000000000155, w1=2.4130777865505686\n",
      "SubSGD iter. 164/499: loss=329343.8622113595, w0=57.750000000000156, w1=2.6866068896814257\n",
      "SubSGD iter. 165/499: loss=321937.4152611546, w0=58.10000000000016, w1=3.0200529494857733\n",
      "SubSGD iter. 166/499: loss=318270.4205484392, w0=58.45000000000016, w1=2.7684225965330422\n",
      "SubSGD iter. 167/499: loss=314852.76346387324, w0=58.80000000000016, w1=2.5572024457015576\n",
      "SubSGD iter. 168/499: loss=312061.33380226896, w0=59.15000000000016, w1=2.3016408722569324\n",
      "SubSGD iter. 169/499: loss=310135.36366442393, w0=59.50000000000016, w1=2.004680409818944\n",
      "SubSGD iter. 170/499: loss=306197.0290913245, w0=59.850000000000165, w1=1.9800291371753203\n",
      "SubSGD iter. 171/499: loss=303853.53395598056, w0=60.200000000000166, w1=1.8063848188977238\n",
      "SubSGD iter. 172/499: loss=296736.0317317651, w0=60.55000000000017, w1=2.132546741072819\n",
      "SubSGD iter. 173/499: loss=290705.4406615703, w0=60.90000000000017, w1=2.3502909963711427\n",
      "SubSGD iter. 174/499: loss=286043.0158666041, w0=61.25000000000017, w1=2.4421963235348847\n",
      "SubSGD iter. 175/499: loss=284163.46387132624, w0=60.90000000000017, w1=3.0343221396306763\n",
      "SubSGD iter. 176/499: loss=281832.984700735, w0=61.25000000000017, w1=2.8606778213530797\n",
      "SubSGD iter. 177/499: loss=271453.60524743394, w0=61.60000000000017, w1=3.5306101149083973\n",
      "SubSGD iter. 178/499: loss=271101.62203880935, w0=61.95000000000017, w1=3.1789129730153434\n",
      "SubSGD iter. 179/499: loss=272835.6386948422, w0=61.60000000000017, w1=3.3872729965542216\n",
      "SubSGD iter. 180/499: loss=271145.9371834592, w0=61.95000000000017, w1=3.174512533410854\n",
      "SubSGD iter. 181/499: loss=264255.9686672211, w0=62.300000000000175, w1=3.492580890161449\n",
      "SubSGD iter. 182/499: loss=257221.08363543256, w0=62.650000000000176, w1=3.831933217604436\n",
      "SubSGD iter. 183/499: loss=250776.69889314938, w0=63.00000000000018, w1=4.114761615777476\n",
      "SubSGD iter. 184/499: loss=246428.57136701222, w0=63.35000000000018, w1=4.196596354431667\n",
      "SubSGD iter. 185/499: loss=237616.78173819958, w0=63.70000000000018, w1=4.710668548928243\n",
      "SubSGD iter. 186/499: loss=240882.64023236293, w0=63.35000000000018, w1=4.735319821571867\n",
      "SubSGD iter. 187/499: loss=234852.6054221071, w0=63.70000000000018, w1=4.976921840032631\n",
      "SubSGD iter. 188/499: loss=236182.87357882518, w0=64.05000000000018, w1=4.532624200567897\n",
      "SubSGD iter. 189/499: loss=229451.23403015363, w0=64.40000000000018, w1=4.851175382084336\n",
      "SubSGD iter. 190/499: loss=232924.07724135404, w0=64.05000000000018, w1=4.836989392510922\n",
      "SubSGD iter. 191/499: loss=228289.59086050003, w0=64.40000000000018, w1=4.958694719586282\n",
      "SubSGD iter. 192/499: loss=226422.56773072926, w0=64.75000000000017, w1=4.8395868118889895\n",
      "SubSGD iter. 193/499: loss=227720.01447043757, w0=64.40000000000018, w1=5.011779501941005\n",
      "SubSGD iter. 194/499: loss=224143.11733655096, w0=64.75000000000017, w1=5.043824027799819\n",
      "SubSGD iter. 195/499: loss=219595.49688612964, w0=65.10000000000016, w1=5.173568987903401\n",
      "SubSGD iter. 196/499: loss=218078.13346626586, w0=65.45000000000016, w1=5.054461080206109\n",
      "SubSGD iter. 197/499: loss=219049.04309433541, w0=65.10000000000016, w1=5.221970553929715\n",
      "SubSGD iter. 198/499: loss=217505.95352338834, w0=65.45000000000016, w1=5.103247887674095\n",
      "SubSGD iter. 199/499: loss=210984.5997509849, w0=65.80000000000015, w1=5.426516902674948\n",
      "SubSGD iter. 200/499: loss=211801.43192947653, w0=65.45000000000016, w1=5.598709592726963\n",
      "SubSGD iter. 201/499: loss=206969.7624902258, w0=65.80000000000015, w1=5.774562701267355\n",
      "SubSGD iter. 202/499: loss=209998.56475454808, w0=66.15000000000015, w1=5.2842448997449845\n",
      "SubSGD iter. 203/499: loss=205265.83279630655, w0=66.50000000000014, w1=5.460098008285376\n",
      "SubSGD iter. 204/499: loss=208432.48316684304, w0=66.85000000000014, w1=5.0332555752838015\n",
      "SubSGD iter. 205/499: loss=204697.96980473778, w0=67.20000000000013, w1=5.151652593509363\n",
      "SubSGD iter. 206/499: loss=199122.67153024673, w0=67.55000000000013, w1=5.4093539156162365\n",
      "SubSGD iter. 207/499: loss=198998.11326965434, w0=67.20000000000013, w1=5.582998233893833\n",
      "SubSGD iter. 208/499: loss=194452.6525505569, w0=67.55000000000013, w1=5.762764973421473\n",
      "SubSGD iter. 209/499: loss=192137.0316069472, w0=67.20000000000013, w1=6.1261302482450795\n",
      "SubSGD iter. 210/499: loss=185147.19734803092, w0=67.55000000000013, w1=6.497634637819951\n",
      "SubSGD iter. 211/499: loss=185417.78902146465, w0=67.90000000000012, w1=6.286635655656824\n",
      "SubSGD iter. 212/499: loss=182092.54101225437, w0=68.25000000000011, w1=6.381820697584559\n",
      "SubSGD iter. 213/499: loss=181588.43466311408, w0=67.90000000000012, w1=6.590180721123438\n",
      "SubSGD iter. 214/499: loss=172186.5408478754, w0=68.25000000000011, w1=7.173951372749196\n",
      "SubSGD iter. 215/499: loss=174989.57385328514, w0=68.60000000000011, w1=6.776286527550852\n",
      "SubSGD iter. 216/499: loss=174279.8460791497, w0=68.9500000000001, w1=6.688855107531679\n",
      "SubSGD iter. 217/499: loss=168712.94554317425, w0=69.3000000000001, w1=6.971683505704719\n",
      "SubSGD iter. 218/499: loss=164687.47214579576, w0=69.65000000000009, w1=7.145178365861355\n",
      "SubSGD iter. 219/499: loss=163586.21585549228, w0=69.3000000000001, w1=7.367446453105569\n",
      "SubSGD iter. 220/499: loss=164898.3937985319, w0=69.65000000000009, w1=7.128998384615047\n",
      "SubSGD iter. 221/499: loss=155145.52091644888, w0=70.00000000000009, w1=7.759583073362366\n",
      "SubSGD iter. 222/499: loss=151715.0772930433, w0=70.35000000000008, w1=7.896723490509548\n",
      "SubSGD iter. 223/499: loss=154322.07658940452, w0=70.70000000000007, w1=7.572552569265926\n",
      "SubSGD iter. 224/499: loss=155704.88045070932, w0=71.05000000000007, w1=7.361553587102799\n",
      "SubSGD iter. 225/499: loss=149078.93378938036, w0=71.40000000000006, w1=7.766550710655796\n",
      "SubSGD iter. 226/499: loss=149317.26390795363, w0=71.05000000000007, w1=7.850034021595395\n",
      "SubSGD iter. 227/499: loss=150918.23497545221, w0=70.70000000000007, w1=7.835848032021981\n",
      "SubSGD iter. 228/499: loss=148461.5172341805, w0=70.35000000000008, w1=8.162245390483548\n",
      "SubSGD iter. 229/499: loss=143975.44303949142, w0=70.00000000000009, w1=8.67905119863514\n",
      "SubSGD iter. 230/499: loss=140519.16837758053, w0=69.65000000000009, w1=9.13640065364052\n",
      "SubSGD iter. 231/499: loss=140651.22892851592, w0=69.3000000000001, w1=9.303910127364125\n",
      "SubSGD iter. 232/499: loss=139474.33321406782, w0=68.9500000000001, w1=9.630307485825693\n",
      "SubSGD iter. 233/499: loss=138764.93653768813, w0=68.60000000000011, w1=9.98701116482564\n",
      "SubSGD iter. 234/499: loss=134021.03695296997, w0=68.9500000000001, w1=10.206959243041792\n",
      "SubSGD iter. 235/499: loss=133496.64934912845, w0=69.3000000000001, w1=9.981743502602432\n",
      "SubSGD iter. 236/499: loss=133365.8119471725, w0=68.9500000000001, w1=10.278703965040421\n",
      "SubSGD iter. 237/499: loss=132938.86723812605, w0=69.3000000000001, w1=10.0402558965499\n",
      "SubSGD iter. 238/499: loss=124792.8635440976, w0=69.65000000000009, w1=10.643113936930458\n",
      "SubSGD iter. 239/499: loss=121873.40246424047, w0=70.00000000000009, w1=10.705536072388632\n",
      "SubSGD iter. 240/499: loss=118201.02873355814, w0=70.35000000000008, w1=10.87432789681187\n",
      "SubSGD iter. 241/499: loss=117622.41672807901, w0=70.00000000000009, w1=11.200725255273438\n",
      "SubSGD iter. 242/499: loss=118451.59234729226, w0=69.65000000000009, w1=11.43131681124645\n",
      "SubSGD iter. 243/499: loss=115017.99812042178, w0=70.00000000000009, w1=11.526501853174185\n",
      "SubSGD iter. 244/499: loss=114394.89111168173, w0=70.35000000000008, w1=11.301286112734825\n",
      "SubSGD iter. 245/499: loss=116335.4775015648, w0=70.70000000000007, w1=10.856988473270091\n",
      "SubSGD iter. 246/499: loss=109950.99525142855, w0=71.05000000000007, w1=11.32760042020727\n",
      "SubSGD iter. 247/499: loss=107188.07220392578, w0=71.40000000000006, w1=11.419408472828241\n",
      "SubSGD iter. 248/499: loss=103820.80676829847, w0=71.75000000000006, w1=11.588200297251479\n",
      "SubSGD iter. 249/499: loss=100105.77035134925, w0=71.40000000000006, w1=12.261525120956435\n",
      "SubSGD iter. 250/499: loss=95799.95526813735, w0=71.75000000000006, w1=12.584794135957289\n",
      "SubSGD iter. 251/499: loss=93975.50380703215, w0=72.10000000000005, w1=12.59668016545319\n",
      "SubSGD iter. 252/499: loss=92437.656209665, w0=72.45000000000005, w1=12.608566194949091\n",
      "SubSGD iter. 253/499: loss=92323.78321904977, w0=72.80000000000004, w1=12.477999287036239\n",
      "SubSGD iter. 254/499: loss=94716.38402357012, w0=72.45000000000005, w1=12.298232547508599\n",
      "SubSGD iter. 255/499: loss=93082.44093511208, w0=72.80000000000004, w1=12.365955592229978\n",
      "SubSGD iter. 256/499: loss=88543.31242234497, w0=73.15000000000003, w1=13.003454623929144\n",
      "SubSGD iter. 257/499: loss=88472.90736879036, w0=72.80000000000004, w1=13.140907929029895\n",
      "SubSGD iter. 258/499: loss=86939.10416785716, w0=73.15000000000003, w1=13.36773346536096\n",
      "SubSGD iter. 259/499: loss=85398.18507607428, w0=73.50000000000003, w1=13.838078146505895\n",
      "SubSGD iter. 260/499: loss=85759.58946639989, w0=73.85000000000002, w1=13.803075890043344\n",
      "SubSGD iter. 261/499: loss=85733.75791855872, w0=74.20000000000002, w1=14.184130578335928\n",
      "SubSGD iter. 262/499: loss=84893.9140515966, w0=73.85000000000002, w1=14.31243371489778\n",
      "SubSGD iter. 263/499: loss=85557.6969495109, w0=74.20000000000002, w1=14.91529175527834\n",
      "SubSGD iter. 264/499: loss=84775.62409692667, w0=73.85000000000002, w1=14.94261714009653\n",
      "SubSGD iter. 265/499: loss=84118.5997690836, w0=73.50000000000003, w1=14.812872179992947\n",
      "SubSGD iter. 266/499: loss=84593.41315206117, w0=73.85000000000002, w1=14.587656439553587\n",
      "SubSGD iter. 267/499: loss=85496.8960535012, w0=74.20000000000002, w1=14.870484837726627\n",
      "SubSGD iter. 268/499: loss=84600.2468491512, w0=73.85000000000002, w1=14.72759611569336\n",
      "SubSGD iter. 269/499: loss=84287.09561639467, w0=73.50000000000003, w1=14.942357489954102\n",
      "SubSGD iter. 270/499: loss=84446.34192236821, w0=73.15000000000003, w1=14.311772801206782\n",
      "SubSGD iter. 271/499: loss=84140.36203803569, w0=73.50000000000003, w1=14.692827489499367\n",
      "SubSGD iter. 272/499: loss=85166.14387991039, w0=73.85000000000002, w1=14.134357240899881\n",
      "SubSGD iter. 273/499: loss=85352.12544929399, w0=73.50000000000003, w1=13.85868006023589\n",
      "SubSGD iter. 274/499: loss=85222.06736114799, w0=73.85000000000002, w1=14.100282078696655\n",
      "SubSGD iter. 275/499: loss=84347.19298378094, w0=73.50000000000003, w1=14.399173398725392\n",
      "SubSGD iter. 276/499: loss=84631.22816016643, w0=73.85000000000002, w1=14.787878197162792\n",
      "SubSGD iter. 277/499: loss=84312.42733554046, w0=73.50000000000003, w1=14.960070887214808\n",
      "SubSGD iter. 278/499: loss=84797.91057284255, w0=73.85000000000002, w1=14.963712722377187\n",
      "SubSGD iter. 279/499: loss=84960.04729694374, w0=73.50000000000003, w1=15.281736063621077\n",
      "SubSGD iter. 280/499: loss=84978.10557190729, w0=73.15000000000003, w1=15.365219374560676\n",
      "SubSGD iter. 281/499: loss=84565.2850946211, w0=72.80000000000004, w1=14.719996583938503\n",
      "SubSGD iter. 282/499: loss=85414.3950593234, w0=72.45000000000005, w1=14.488329419029464\n",
      "SubSGD iter. 283/499: loss=84593.44433427032, w0=72.80000000000004, w1=14.869384107322048\n",
      "SubSGD iter. 284/499: loss=85487.8649457023, w0=72.45000000000005, w1=14.300003258544924\n",
      "SubSGD iter. 285/499: loss=87377.87684012369, w0=72.10000000000005, w1=13.928498868970053\n",
      "SubSGD iter. 286/499: loss=85529.37150927412, w0=72.45000000000005, w1=14.251767883970906\n",
      "SubSGD iter. 287/499: loss=85571.75872847279, w0=72.80000000000004, w1=13.917118933781957\n",
      "SubSGD iter. 288/499: loss=85469.3103027721, w0=72.45000000000005, w1=14.341068740156333\n",
      "SubSGD iter. 289/499: loss=84686.16804533738, w0=72.80000000000004, w1=14.432876792777304\n",
      "SubSGD iter. 290/499: loss=86716.83811992359, w0=72.45000000000005, w1=13.781402855824336\n",
      "SubSGD iter. 291/499: loss=85755.02873026088, w0=72.80000000000004, w1=13.84382499128251\n",
      "SubSGD iter. 292/499: loss=86911.88037715244, w0=72.45000000000005, w1=13.714080031178927\n",
      "SubSGD iter. 293/499: loss=87700.69724169285, w0=72.80000000000004, w1=13.321042322779476\n",
      "SubSGD iter. 294/499: loss=87912.7414063885, w0=72.45000000000005, w1=13.44015023047677\n",
      "SubSGD iter. 295/499: loss=86311.6424947471, w0=72.80000000000004, w1=13.671817395385808\n",
      "SubSGD iter. 296/499: loss=85433.98837007732, w0=73.15000000000003, w1=13.80895781253299\n",
      "SubSGD iter. 297/499: loss=85156.68495229988, w0=73.50000000000003, w1=13.946098229680171\n",
      "SubSGD iter. 298/499: loss=84951.49957532863, w0=73.85000000000002, w1=14.270737782995322\n",
      "SubSGD iter. 299/499: loss=85421.48430995239, w0=74.20000000000002, w1=14.814910528836785\n",
      "SubSGD iter. 300/499: loss=84602.75509064093, w0=73.85000000000002, w1=14.733075790182594\n",
      "SubSGD iter. 301/499: loss=84169.04541862535, w0=73.50000000000003, w1=14.639928386187004\n",
      "SubSGD iter. 302/499: loss=84896.03381489692, w0=73.15000000000003, w1=14.009343697439684\n",
      "SubSGD iter. 303/499: loss=84828.45828353862, w0=72.80000000000004, w1=14.32966509824564\n",
      "SubSGD iter. 304/499: loss=85393.87426500353, w0=72.45000000000005, w1=14.753614904620015\n",
      "SubSGD iter. 305/499: loss=84786.33387894249, w0=72.80000000000004, w1=15.1346695929126\n",
      "SubSGD iter. 306/499: loss=84398.11550299976, w0=73.15000000000003, w1=15.0620588125553\n",
      "SubSGD iter. 307/499: loss=85220.33018407675, w0=72.80000000000004, w1=15.371180336840725\n",
      "SubSGD iter. 308/499: loss=86306.00598910148, w0=73.15000000000003, w1=15.82876164328802\n",
      "SubSGD iter. 309/499: loss=87238.24004986546, w0=73.50000000000003, w1=16.070363661748786\n",
      "SubSGD iter. 310/499: loss=86857.19542757348, w0=73.85000000000002, w1=15.83384542164312\n",
      "SubSGD iter. 311/499: loss=85752.60993842414, w0=73.50000000000003, w1=15.616101166344796\n",
      "SubSGD iter. 312/499: loss=85528.63143238606, w0=73.15000000000003, w1=15.57198802779802\n",
      "SubSGD iter. 313/499: loss=84611.71654545923, w0=73.50000000000003, w1=15.127690388333285\n",
      "SubSGD iter. 314/499: loss=85284.2278033085, w0=73.85000000000002, w1=15.22287543026102\n",
      "SubSGD iter. 315/499: loss=86692.49047545875, w0=74.20000000000002, w1=15.496404533391877\n",
      "SubSGD iter. 316/499: loss=88739.96907578337, w0=74.55000000000001, w1=15.829850593196225\n",
      "SubSGD iter. 317/499: loss=85943.03661705744, w0=74.20000000000002, w1=15.199265904448906\n",
      "SubSGD iter. 318/499: loss=86183.63851637958, w0=73.85000000000002, w1=15.606248770417233\n",
      "SubSGD iter. 319/499: loss=86327.81136736824, w0=73.50000000000003, w1=15.815665796643907\n",
      "SubSGD iter. 320/499: loss=87187.8786364015, w0=73.15000000000003, w1=16.06729614959664\n",
      "SubSGD iter. 321/499: loss=86694.0660547903, w0=72.80000000000004, w1=15.849551894298315\n",
      "SubSGD iter. 322/499: loss=87454.75230955961, w0=72.45000000000005, w1=15.863015590229766\n",
      "SubSGD iter. 323/499: loss=86554.2313093388, w0=72.10000000000005, w1=15.211541653276798\n",
      "SubSGD iter. 324/499: loss=85388.06923552477, w0=72.45000000000005, w1=14.903708394944445\n",
      "SubSGD iter. 325/499: loss=84879.81630989168, w0=72.80000000000004, w1=15.193326397487708\n",
      "SubSGD iter. 326/499: loss=85732.9386848916, w0=72.45000000000005, w1=15.366970715765303\n",
      "SubSGD iter. 327/499: loss=87809.96902957758, w0=72.10000000000005, w1=15.699929059279373\n",
      "SubSGD iter. 328/499: loss=90300.83432713426, w0=71.75000000000006, w1=15.914690433540114\n",
      "SubSGD iter. 329/499: loss=87180.35676362106, w0=72.10000000000005, w1=15.521652725140664\n",
      "SubSGD iter. 330/499: loss=86057.06667547839, w0=72.45000000000005, w1=15.491341643443683\n",
      "SubSGD iter. 331/499: loss=87144.25145097653, w0=72.80000000000004, w1=15.961686324588618\n",
      "SubSGD iter. 332/499: loss=85277.0973125143, w0=73.15000000000003, w1=15.478257290680379\n",
      "SubSGD iter. 333/499: loss=84314.40170850986, w0=73.50000000000003, w1=14.961451482528787\n",
      "SubSGD iter. 334/499: loss=84517.09827247082, w0=73.15000000000003, w1=15.135095800806383\n",
      "SubSGD iter. 335/499: loss=85382.35749833526, w0=73.50000000000003, w1=15.463671568767182\n",
      "SubSGD iter. 336/499: loss=85347.86651501746, w0=73.85000000000002, w1=15.252672586604056\n",
      "SubSGD iter. 337/499: loss=85526.70577126498, w0=74.20000000000002, w1=14.89245296079292\n",
      "SubSGD iter. 338/499: loss=85137.35547271387, w0=73.85000000000002, w1=14.152648798121376\n",
      "SubSGD iter. 339/499: loss=84655.23618451144, w0=73.50000000000003, w1=14.177300070765\n",
      "SubSGD iter. 340/499: loss=84897.97013348601, w0=73.15000000000003, w1=14.008508246341762\n",
      "SubSGD iter. 341/499: loss=84315.14296457217, w0=73.50000000000003, w1=14.432247953658889\n",
      "SubSGD iter. 342/499: loss=84203.30444050115, w0=73.15000000000003, w1=14.551355861356182\n",
      "SubSGD iter. 343/499: loss=84180.35059273281, w0=73.50000000000003, w1=14.619078906077561\n",
      "SubSGD iter. 344/499: loss=84223.44222025941, w0=73.15000000000003, w1=14.932946746868996\n",
      "SubSGD iter. 345/499: loss=84305.0835402302, w0=73.50000000000003, w1=14.442628945346625\n",
      "SubSGD iter. 346/499: loss=84618.62794573719, w0=73.85000000000002, w1=14.534534272510367\n",
      "SubSGD iter. 347/499: loss=85324.68685405972, w0=74.20000000000002, w1=14.499532016047816\n",
      "SubSGD iter. 348/499: loss=85025.28691721917, w0=73.85000000000002, w1=14.223854835383825\n",
      "SubSGD iter. 349/499: loss=85506.01517742616, w0=74.20000000000002, w1=14.356567178242976\n",
      "SubSGD iter. 350/499: loss=86584.21032161961, w0=74.55000000000001, w1=14.73762186653556\n",
      "SubSGD iter. 351/499: loss=85957.7610003491, w0=74.20000000000002, w1=15.208434845261413\n",
      "SubSGD iter. 352/499: loss=86711.14884136849, w0=74.55000000000001, w1=14.969986776770892\n",
      "SubSGD iter. 353/499: loss=87920.70830446407, w0=74.9, w1=14.64581585552727\n",
      "SubSGD iter. 354/499: loss=86644.46804322467, w0=74.55000000000001, w1=14.854175879066148\n",
      "SubSGD iter. 355/499: loss=89090.81615663656, w0=74.9, w1=15.524108172621466\n",
      "SubSGD iter. 356/499: loss=88427.72820832729, w0=74.55000000000001, w1=15.73532832345295\n",
      "SubSGD iter. 357/499: loss=90196.64047043883, w0=74.9, w1=15.865460517039764\n",
      "SubSGD iter. 358/499: loss=91757.33657081595, w0=75.25, w1=15.856918929165605\n",
      "SubSGD iter. 359/499: loss=92410.21181847573, w0=75.6, w1=15.399569474160225\n",
      "SubSGD iter. 360/499: loss=90201.34526523616, w0=75.25, w1=15.269824514056642\n",
      "SubSGD iter. 361/499: loss=88179.60823948897, w0=74.9, w1=14.982873500535653\n",
      "SubSGD iter. 362/499: loss=87230.78502789805, w0=74.55000000000001, w1=15.28176482056439\n",
      "SubSGD iter. 363/499: loss=86682.64021870916, w0=74.20000000000002, w1=15.492984971395876\n",
      "SubSGD iter. 364/499: loss=86618.76561019689, w0=74.55000000000001, w1=14.80446072851962\n",
      "SubSGD iter. 365/499: loss=85320.95364522636, w0=74.20000000000002, w1=14.674715768416037\n",
      "SubSGD iter. 366/499: loss=85078.28143293891, w0=73.85000000000002, w1=15.12644644809921\n",
      "SubSGD iter. 367/499: loss=85443.03424066221, w0=73.50000000000003, w1=15.489811722922818\n",
      "SubSGD iter. 368/499: loss=86115.3016296555, w0=73.85000000000002, w1=15.581619775543789\n",
      "SubSGD iter. 369/499: loss=85760.59161325077, w0=74.20000000000002, w1=15.064813967392197\n",
      "SubSGD iter. 370/499: loss=86671.36427454004, w0=74.55000000000001, w1=14.906200144143815\n",
      "SubSGD iter. 371/499: loss=87944.19112114781, w0=74.9, w1=14.69520116198069\n",
      "SubSGD iter. 372/499: loss=87029.3801208281, w0=74.55000000000001, w1=15.178630195888928\n",
      "SubSGD iter. 373/499: loss=88149.30681520524, w0=74.9, w1=14.953414455449568\n",
      "SubSGD iter. 374/499: loss=88012.57884099595, w0=74.55000000000001, w1=15.594550916585892\n",
      "SubSGD iter. 375/499: loss=89480.17935843774, w0=74.9, w1=15.656973052044066\n",
      "SubSGD iter. 376/499: loss=89232.0051971772, w0=74.55000000000001, w1=15.974996393287956\n",
      "SubSGD iter. 377/499: loss=91148.34109515283, w0=74.9, w1=16.11213681043514\n",
      "SubSGD iter. 378/499: loss=91275.10987645894, w0=75.25, w1=15.719099102035688\n",
      "SubSGD iter. 379/499: loss=94221.80120454007, w0=75.6, w1=16.008717104578952\n",
      "SubSGD iter. 380/499: loss=95434.77746253923, w0=75.94999999999999, w1=15.648497478767815\n",
      "SubSGD iter. 381/499: loss=93104.48996217144, w0=75.6, w1=15.684753490452138\n",
      "SubSGD iter. 382/499: loss=93115.70119486097, w0=75.25, w1=16.194463952921712\n",
      "SubSGD iter. 383/499: loss=89306.44228700343, w0=74.9, w1=15.605843623250898\n",
      "SubSGD iter. 384/499: loss=88687.75567349748, w0=74.55000000000001, w1=15.814203646789776\n",
      "SubSGD iter. 385/499: loss=89043.31512458067, w0=74.9, w1=15.504953743857916\n",
      "SubSGD iter. 386/499: loss=88687.32727771103, w0=74.55000000000001, w1=15.814075268143341\n",
      "SubSGD iter. 387/499: loss=88964.47313469839, w0=74.9, w1=15.47316131296927\n",
      "SubSGD iter. 388/499: loss=86762.33692513718, w0=74.55000000000001, w1=15.023912252379615\n",
      "SubSGD iter. 389/499: loss=87891.60828131292, w0=74.9, w1=14.566562797374235\n",
      "SubSGD iter. 390/499: loss=86630.57901071472, w0=74.55000000000001, w1=14.82731089934104\n",
      "SubSGD iter. 391/499: loss=86011.50189637828, w0=74.20000000000002, w1=15.234293765309367\n",
      "SubSGD iter. 392/499: loss=87665.08782997601, w0=74.55000000000001, w1=15.475895783770131\n",
      "SubSGD iter. 393/499: loss=87988.01832942342, w0=74.9, w1=14.787371540893876\n",
      "SubSGD iter. 394/499: loss=89680.99638428859, w0=75.25, w1=14.924511958041057\n",
      "SubSGD iter. 395/499: loss=87900.79021685323, w0=74.9, w1=14.321653917660498\n",
      "SubSGD iter. 396/499: loss=86522.68416880046, w0=74.55000000000001, w1=14.618614380098487\n",
      "SubSGD iter. 397/499: loss=87951.42886727356, w0=74.9, w1=14.710422432719458\n",
      "SubSGD iter. 398/499: loss=89489.60974176503, w0=75.25, w1=14.559819404074068\n",
      "SubSGD iter. 399/499: loss=87963.07736976512, w0=74.9, w1=14.734919701212265\n",
      "SubSGD iter. 400/499: loss=89586.34297078554, w0=75.25, w1=14.802642745933644\n",
      "SubSGD iter. 401/499: loss=91499.3681059263, w0=75.6, w1=14.806284581096023\n",
      "SubSGD iter. 402/499: loss=94065.08091587921, w0=75.94999999999999, w1=14.936416774682836\n",
      "SubSGD iter. 403/499: loss=91278.96838893808, w0=75.6, w1=14.352646123057077\n",
      "SubSGD iter. 404/499: loss=89599.59940487937, w0=75.25, w1=14.82345910178293\n",
      "SubSGD iter. 405/499: loss=88597.409537561, w0=74.9, w1=15.306117131910778\n",
      "SubSGD iter. 406/499: loss=90325.35491981443, w0=75.25, w1=15.338161657769591\n",
      "SubSGD iter. 407/499: loss=88434.06457904914, w0=74.9, w1=15.216456330694232\n",
      "SubSGD iter. 408/499: loss=88313.04387038862, w0=74.55000000000001, w1=15.696903397336525\n",
      "SubSGD iter. 409/499: loss=89927.82455982492, w0=74.9, w1=15.788711449957496\n",
      "SubSGD iter. 410/499: loss=91848.37093907307, w0=75.25, w1=15.880519502578467\n",
      "SubSGD iter. 411/499: loss=94190.08511796329, w0=75.6, w1=16.0018464313784\n",
      "SubSGD iter. 412/499: loss=93648.46919926695, w0=75.25, w1=16.315714272169835\n",
      "SubSGD iter. 413/499: loss=93878.38444819053, w0=75.6, w1=15.922676563770384\n",
      "SubSGD iter. 414/499: loss=91956.26360717566, w0=75.25, w1=15.908490574196971\n",
      "SubSGD iter. 415/499: loss=90804.1055873982, w0=74.9, w1=16.026325279902913\n",
      "SubSGD iter. 416/499: loss=90713.63309762506, w0=74.55000000000001, w1=16.34664668070887\n",
      "SubSGD iter. 417/499: loss=93885.32657857751, w0=74.9, w1=16.73535147914627\n",
      "SubSGD iter. 418/499: loss=93337.52940082728, w0=74.55000000000001, w1=16.910451776284468\n",
      "SubSGD iter. 419/499: loss=96771.97372660453, w0=74.9, w1=17.23320976869855\n",
      "SubSGD iter. 420/499: loss=95394.16488642152, w0=74.55000000000001, w1=17.24667346463\n",
      "SubSGD iter. 421/499: loss=95913.09488633674, w0=74.20000000000002, w1=17.45503348816888\n",
      "SubSGD iter. 422/499: loss=97817.10354794947, w0=73.85000000000002, w1=17.775354888974835\n",
      "SubSGD iter. 423/499: loss=99095.11136941715, w0=73.50000000000003, w1=17.950455186113032\n",
      "SubSGD iter. 424/499: loss=96231.79547107767, w0=73.15000000000003, w1=17.56046930610772\n",
      "SubSGD iter. 425/499: loss=94912.79321099268, w0=72.80000000000004, w1=17.302767984000845\n",
      "SubSGD iter. 426/499: loss=96113.56065839982, w0=73.15000000000003, w1=17.54437000246161\n",
      "SubSGD iter. 427/499: loss=94197.52553961283, w0=73.50000000000003, w1=17.315529027447532\n",
      "SubSGD iter. 428/499: loss=91073.0985704108, w0=73.85000000000002, w1=16.805818564977958\n",
      "SubSGD iter. 429/499: loss=92477.78201748131, w0=73.50000000000003, w1=17.066566666944762\n",
      "SubSGD iter. 430/499: loss=92490.08255786938, w0=73.85000000000002, w1=17.03031065526044\n",
      "SubSGD iter. 431/499: loss=93423.22664311211, w0=74.20000000000002, w1=17.092732790718614\n",
      "SubSGD iter. 432/499: loss=95865.5331457942, w0=74.55000000000001, w1=17.312680868934766\n",
      "SubSGD iter. 433/499: loss=91901.90920912991, w0=74.20000000000002, w1=16.842378122980932\n",
      "SubSGD iter. 434/499: loss=93468.4461999272, w0=74.55000000000001, w1=16.934283450144676\n",
      "SubSGD iter. 435/499: loss=92729.52847918234, w0=74.9, w1=16.489985810679944\n",
      "SubSGD iter. 436/499: loss=95136.29135620262, w0=75.25, w1=16.622698153539094\n",
      "SubSGD iter. 437/499: loss=98033.57322405357, w0=75.6, w1=16.741095171764655\n",
      "SubSGD iter. 438/499: loss=95090.93747882388, w0=75.25, w1=16.613607832990674\n",
      "SubSGD iter. 439/499: loss=92642.66391682293, w0=74.9, w1=16.470719110957408\n",
      "SubSGD iter. 440/499: loss=92403.6049173707, w0=74.55000000000001, w1=16.73174816211082\n",
      "SubSGD iter. 441/499: loss=92491.45206082074, w0=74.20000000000002, w1=16.942968312942302\n",
      "SubSGD iter. 442/499: loss=91373.98624724391, w0=74.55000000000001, w1=16.49867067347757\n",
      "SubSGD iter. 443/499: loss=91753.62148104102, w0=74.20000000000002, w1=16.81669401472146\n",
      "SubSGD iter. 444/499: loss=90901.30408259259, w0=74.55000000000001, w1=16.389851581719885\n",
      "SubSGD iter. 445/499: loss=91974.41858455367, w0=74.20000000000002, w1=16.85481962193366\n",
      "SubSGD iter. 446/499: loss=93760.9613264538, w0=74.55000000000001, w1=16.98753196479281\n",
      "SubSGD iter. 447/499: loss=94628.15356334101, w0=74.20000000000002, w1=17.269543087186236\n",
      "SubSGD iter. 448/499: loss=93331.33112129483, w0=74.55000000000001, w1=16.9093234613751\n",
      "SubSGD iter. 449/499: loss=90587.66172737791, w0=74.20000000000002, w1=16.598025463798123\n",
      "SubSGD iter. 450/499: loss=88155.27526811432, w0=73.85000000000002, w1=16.213169981628038\n",
      "SubSGD iter. 451/499: loss=88825.60577502812, w0=73.50000000000003, w1=16.422587007854712\n",
      "SubSGD iter. 452/499: loss=87528.94946332145, w0=73.15000000000003, w1=16.145997907881714\n",
      "SubSGD iter. 453/499: loss=89177.44429784967, w0=73.50000000000003, w1=16.49040948551357\n",
      "SubSGD iter. 454/499: loss=90984.87518899607, w0=73.15000000000003, w1=16.787369947951557\n",
      "SubSGD iter. 455/499: loss=87932.45927961086, w0=72.80000000000004, w1=16.13589601099859\n",
      "SubSGD iter. 456/499: loss=86460.4585060408, w0=73.15000000000003, w1=15.875147909031785\n",
      "SubSGD iter. 457/499: loss=88592.81434783248, w0=72.80000000000004, w1=16.26943401258498\n",
      "SubSGD iter. 458/499: loss=91632.88050562731, w0=72.45000000000005, w1=16.663720116138176\n",
      "SubSGD iter. 459/499: loss=88896.35233168842, w0=72.80000000000004, w1=16.329071165949227\n",
      "SubSGD iter. 460/499: loss=89716.11763998047, w0=72.45000000000005, w1=16.32263605301972\n",
      "SubSGD iter. 461/499: loss=88022.88533615874, w0=72.10000000000005, w1=15.753255204242597\n",
      "SubSGD iter. 462/499: loss=89026.23488248633, w0=71.75000000000006, w1=15.573488464714957\n",
      "SubSGD iter. 463/499: loss=92030.75078901755, w0=71.40000000000006, w1=15.829859871855176\n",
      "SubSGD iter. 464/499: loss=90699.34626963832, w0=71.75000000000006, w1=16.00335473201181\n",
      "SubSGD iter. 465/499: loss=89041.76723450146, w0=72.10000000000005, w1=15.97602934719362\n",
      "SubSGD iter. 466/499: loss=88262.33250773475, w0=72.45000000000005, w1=16.038451482651794\n",
      "SubSGD iter. 467/499: loss=89122.78460407542, w0=72.80000000000004, w1=16.371897542456143\n",
      "SubSGD iter. 468/499: loss=87516.20253824806, w0=73.15000000000003, w1=16.143056567442066\n",
      "SubSGD iter. 469/499: loss=88564.49048969398, w0=73.50000000000003, w1=16.36988210377313\n",
      "SubSGD iter. 470/499: loss=86214.7017566935, w0=73.15000000000003, w1=15.80134051430977\n",
      "SubSGD iter. 471/499: loss=87175.6716816346, w0=72.80000000000004, w1=15.968849988033375\n",
      "SubSGD iter. 472/499: loss=86651.73688430968, w0=73.15000000000003, w1=15.932593976349052\n",
      "SubSGD iter. 473/499: loss=88522.30654116801, w0=72.80000000000004, w1=16.255581100913073\n",
      "SubSGD iter. 474/499: loss=90039.45384912132, w0=72.45000000000005, w1=16.383884237474923\n",
      "SubSGD iter. 475/499: loss=88993.61582605077, w0=72.80000000000004, w1=16.3476282257906\n",
      "SubSGD iter. 476/499: loss=90440.22864278333, w0=73.15000000000003, w1=16.69818561350979\n",
      "SubSGD iter. 477/499: loss=88005.11183965609, w0=73.50000000000003, w1=16.253887974045057\n",
      "SubSGD iter. 478/499: loss=88311.48219077145, w0=73.85000000000002, w1=16.25147976004836\n",
      "SubSGD iter. 479/499: loss=87913.36500190165, w0=74.20000000000002, w1=15.899782618155305\n",
      "SubSGD iter. 480/499: loss=87144.83764285836, w0=73.85000000000002, w1=15.927108002973496\n",
      "SubSGD iter. 481/499: loss=84740.85043805838, w0=73.50000000000003, w1=15.187303840301952\n",
      "SubSGD iter. 482/499: loss=86829.91596466208, w0=73.85000000000002, w1=15.824802872001118\n",
      "SubSGD iter. 483/499: loss=86452.85933764881, w0=73.50000000000003, w1=15.85585224646552\n",
      "SubSGD iter. 484/499: loss=87362.03517331123, w0=73.15000000000003, w1=16.10748259941825\n",
      "SubSGD iter. 485/499: loss=85940.36978289403, w0=72.80000000000004, w1=15.636870652481072\n",
      "SubSGD iter. 486/499: loss=87681.19209400733, w0=73.15000000000003, w1=16.181043398322537\n",
      "SubSGD iter. 487/499: loss=89336.36983254163, w0=73.50000000000003, w1=16.520395725765525\n",
      "SubSGD iter. 488/499: loss=87782.00072252673, w0=73.85000000000002, w1=16.117248741779587\n",
      "SubSGD iter. 489/499: loss=85401.74975993307, w0=73.50000000000003, w1=15.472025951157415\n",
      "SubSGD iter. 490/499: loss=85327.60353085998, w0=73.85000000000002, w1=15.243184976143338\n",
      "SubSGD iter. 491/499: loss=85331.9007190089, w0=74.20000000000002, w1=14.70316808664331\n",
      "SubSGD iter. 492/499: loss=85011.59225530832, w0=73.85000000000002, w1=14.23255613970613\n",
      "SubSGD iter. 493/499: loss=85331.78175344865, w0=74.20000000000002, w1=14.702858885659966\n",
      "SubSGD iter. 494/499: loss=86421.2580303729, w0=74.55000000000001, w1=14.393608982728106\n",
      "SubSGD iter. 495/499: loss=88009.35025938485, w0=74.9, w1=14.817348690045232\n",
      "SubSGD iter. 496/499: loss=86949.3917931965, w0=74.55000000000001, w1=15.137670090851188\n",
      "SubSGD iter. 497/499: loss=88538.11577358203, w0=74.9, w1=15.27481050799837\n",
      "SubSGD iter. 498/499: loss=90217.00196448971, w0=75.25, w1=15.278452343160748\n",
      "SubSGD iter. 499/499: loss=88026.02102503336, w0=74.9, w1=14.833556008627337\n",
      "SubSGD: execution time=0.062 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46985ecb215e40ed8fbec0d2c20ce8d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses,\n",
    "        subsgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
